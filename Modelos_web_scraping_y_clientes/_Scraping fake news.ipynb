{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import copy\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# El Mundo Today"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.elmundotoday.com/'\n",
    "peticion = requests.get(url, allow_redirects = False)\n",
    "soup = BeautifulSoup(peticion.text, \"lxml\")\n",
    "\n",
    "# Inicializamos listas vacías para llenarlas con bucles\n",
    "web = [] \n",
    "titulo = [] \n",
    "por = [] \n",
    "cab = [] \n",
    "com = []\n",
    "texto = [] \n",
    "pag = [] \n",
    "url = [] \n",
    "\n",
    "\n",
    "for i in range(0, len(soup.findAll('h3'))):\n",
    "    web.append('El Mundo Today')\n",
    "    titulo.append(soup.findAll('h3')[i].text)\n",
    "    \n",
    "    # Cabecera: si es la primera noticia que ponga un 1, para el resto, 0\n",
    "    if i == 0:\n",
    "        cab.append(1)\n",
    "    else:\n",
    "        cab.append(0)\n",
    "    \n",
    "    # Comentarios = None, no hay comentarios en esta página\n",
    "    com.append(None)\n",
    "    \n",
    "    # Página siempre a 1\n",
    "    pag.append(1)\n",
    "    \n",
    "    # URL\n",
    "    urll = soup.findAll('h3')[i].find('a').get('href')\n",
    "    url.append(urll)\n",
    "    \n",
    "    # Autor\n",
    "    peticion_noticia = requests.get(urll, allow_redirects = False)\n",
    "    soup_noticia = BeautifulSoup(peticion_noticia.text, \"lxml\")\n",
    "    try:\n",
    "        por.append(soup_noticia.findAll('a', {'class' : 'tdb-author-name'})[0].text)\n",
    "    except:\n",
    "        por.append('Revisar')\n",
    "    \n",
    "    # Texto\n",
    "    try:\n",
    "        tex = []\n",
    "\n",
    "        for i in range(0, len(soup_noticia.findAll('p', {'class': ''}))):\n",
    "            tex.append(soup_noticia.findAll('p', {'class': ''})[i].text)\n",
    "\n",
    "        tex2 = ' '.join(tex)\n",
    "        texto.append(tex2.split('(function($)')[0])\n",
    "    except:\n",
    "        texto.append(None) # Hay muchos vídeos, que no tienen autor declarado en la web\n",
    "\n",
    "    \n",
    "\n",
    "table_dict = {'Web': web,\\\n",
    "              'Título': titulo,\\\n",
    "              'Autor': por,\\\n",
    "              'Cabecera': cab,\\\n",
    "              'Comentarios': com,\\\n",
    "              'Texto': texto,\\\n",
    "              'Página' : pag,\\\n",
    "              'URL': url}\n",
    "\n",
    "emt = pd.DataFrame(table_dict) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maldita "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Web que se dedica a destapar bulos. Como la mayoría de sus artículos son como: \"No, el Hospital Arnau de Vilanova de Lleida no está vacío y la carpa que se ha instalado no es un hospital de campaña, funciona como una sala de espera para pacientes con síntomas de COVID-19\", es decir, con la estructura \"**No,** esto **no** es cierto\", la estrategia es sacar todos los titulares eliminado los \"no/ni\" para quedarnos con una aproximación de los titulares de las noticias falsas.  \n",
    "Noticia real: \"No, esto no es cierto\"  \n",
    "Noticia falsa: \"~~No,~~ esto ~~no~~ es cierto\" --> \"esto es cierto\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://maldita.es/malditobulo/'\n",
    "peticion = requests.get(url)\n",
    "soup = BeautifulSoup(peticion.text, \"lxml\")\n",
    "\n",
    "# Inicializamos listas vacías para llenarlas con bucles\n",
    "web = [] \n",
    "titulo = [] \n",
    "por = [] \n",
    "cab = [] \n",
    "com = [] \n",
    "texto = [] \n",
    "pag = [] \n",
    "url = [] \n",
    "\n",
    "for i in range(0, len(soup.findAll('article'))):\n",
    "    web.append('Maldita')\n",
    "    \n",
    "    try:\n",
    "        titulo.append(re.sub(r\"\\bno\\b\", '', soup.findAll('article')[i]\\\n",
    "                             .find('h2').text.split(':')[0]\\\n",
    "                             .replace('No, ', '').replace(' no ', ' ').replace('No ', '').replace(' ni ',' ')))\n",
    "    except:\n",
    "        titulo.append('Revisar')\n",
    "    \n",
    "    por.append('Autor')\n",
    "    cab.append('0')\n",
    "    pag.append(1)\n",
    "    url.append(None)\n",
    "    com.append(None)\n",
    "    texto.append(None)\n",
    "    \n",
    "\n",
    "# Hacemos un diccionario con todos los resultados obtenidos y lo pasamos a formato dataframe\n",
    "table_dict = {'Web': web,\\\n",
    "              'Título': titulo,\\\n",
    "              'Autor': por,\\\n",
    "              'Cabecera': cab,\\\n",
    "              'Comentarios': com,\\\n",
    "              'Texto': texto,\\\n",
    "              'Página' : pag,\\\n",
    "              'URL': url}\n",
    "mb = pd.DataFrame(table_dict)\n",
    "mb = mb[mb['Título'] != 'Revisar']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Newtral/bulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.newtral.es/tag/bulo/'\n",
    "peticion = requests.get(url)\n",
    "soup = BeautifulSoup(peticion.text, \"lxml\")\n",
    "\n",
    "# Inicializamos listas vacías para llenarlas con bucles\n",
    "web = [] \n",
    "titulo = [] \n",
    "por = [] \n",
    "cab = [] \n",
    "com = [] \n",
    "texto = [] \n",
    "pag = [] \n",
    "url = [] \n",
    "\n",
    "\n",
    "for i in range(0, len(soup.findAll('article'))):\n",
    "    web.append('Newtral/bulos')\n",
    "    por.append('Newtral/bulos')\n",
    "    cab.append(None)\n",
    "    com.append(None)\n",
    "    pag.append(None)\n",
    "    \n",
    "    url.append(soup.findAll('article')[i].find('h3').find('a').get('href'))\n",
    "    \n",
    "    tex = soup.findAll('article')[i].find('h3').text\\\n",
    "                  .replace('No, ', '').replace(' no ', ' ').replace('No ', '').replace(' ni ',' ').replace('falsas', '')\\\n",
    "                 .replace('falsos', '').replace('falsa', '').replace('falso', '')\n",
    "    titulo.append(tex)\n",
    "    texto.append(tex)\n",
    "\n",
    "\n",
    "# Hacemos un diccionario con todos los resultados obtenidos y lo pasamos a formato dataframe\n",
    "table_dict = {'Web': web,\\\n",
    "              'Título': titulo,\\\n",
    "              'Autor': por,\\\n",
    "              'Cabecera': cab,\\\n",
    "              'Comentarios': com,\\\n",
    "              'Texto': texto,\\\n",
    "              'Página' : pag,\\\n",
    "              'URL': url}\n",
    "nb = pd.DataFrame(table_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Newtral/fakes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No lo sacamos con BeautifulSoup sino con Selenium (webdriver)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ladyl\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:15: DeprecationWarning: use options instead of chrome_options\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "CHROMEDRIVER_PATH = r'C:\\Users\\ladyl\\Downloads\\chromedriver_win32\\chromedriver.exe'\n",
    "CHROME_PATH = \"\"\n",
    "WINDOW_SIZE = \"1920,1080\"\n",
    "\n",
    "chrome_options = Options()\n",
    "# chrome_options.add_argument(\"--headless\")  \n",
    "chrome_options.add_argument(\"--window-size=%s\" % WINDOW_SIZE)\n",
    "chrome_options.binary_location = CHROME_PATH\n",
    "prefs = {'profile.managed_default_content_settings.images':2}\n",
    "chrome_options.add_experimental_option(\"prefs\", prefs)\n",
    "\n",
    "url = \"https://www.newtral.es/fakes/\"\n",
    "\n",
    "driver = webdriver.Chrome(executable_path=CHROMEDRIVER_PATH,\n",
    "    chrome_options=chrome_options\n",
    "    )\n",
    "driver.get(url)\n",
    "\n",
    "final = 50\n",
    "inicial = 1\n",
    "\n",
    "titular = []\n",
    "\n",
    "for i in range(inicial, final):\n",
    "\n",
    "    try:\n",
    "        for j in range(inicial, inicial + 15):\n",
    "            titular.append(driver.find_element_by_xpath('//*[@id=\"main\"]/section/div[' + str(j) + ']/article/div[2]/blockquote/div').text)\n",
    "\n",
    "        html = driver.find_element_by_tag_name('html')\n",
    "        html.send_keys(Keys.END)    \n",
    "\n",
    "        wait = WebDriverWait(driver, 5)\n",
    "        time.sleep(5)\n",
    "\n",
    "        inicial = inicial + 15\n",
    "\n",
    "    except:\n",
    "        titular.append(None)\n",
    "\n",
    "web = ['Newtral/Fakes'] * len(titular)\n",
    "por = [None] * len(titular)\n",
    "cab = [None] * len(titular)\n",
    "com = [None] * len(titular)\n",
    "texto = titular\n",
    "pag = [None] * len(titular)\n",
    "url = [None] * len(titular)\n",
    "\n",
    "table_dict = {'Web': web,\\\n",
    "              'Título': titular,\\\n",
    "              'Autor': por,\\\n",
    "              'Cabecera': cab,\\\n",
    "              'Comentarios': com,\\\n",
    "              'Texto': texto,\\\n",
    "              'Página' : pag,\\\n",
    "              'URL': url}\n",
    "\n",
    "nf = pd.DataFrame(table_dict)\n",
    "\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## El Jueves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.eljueves.es/news'\n",
    "\n",
    "# last_page = 70 # en total son 70 páginas\n",
    "last_page = 2\n",
    "\n",
    "# Hacer lista con todas las url de cada página de noticias \n",
    "list_urls = [url] # la inicializamos con la propia home \n",
    "\n",
    "for i in range(2, last_page+1): # range 2 porque la 1 es la home\n",
    "    list_urls.append(url + '?_page=' + str(i))\n",
    "\n",
    "\n",
    "# Inicializamos listas vacías para llenarlas con bucles\n",
    "web = [] \n",
    "titulo = [] \n",
    "por = [] \n",
    "cab = [] \n",
    "com = []\n",
    "texto = [] \n",
    "pag = [] \n",
    "url = [] \n",
    "\n",
    "for n, p in enumerate(list_urls):\n",
    "    \n",
    "    peticion = requests.get(p, allow_redirects = False)\n",
    "    soup = BeautifulSoup(peticion.text, \"lxml\")\n",
    "\n",
    "    for i in range(0, len(soup.findAll('article'))):\n",
    "        web.append('El Jueves')\n",
    "\n",
    "        # Titular\n",
    "        try:\n",
    "            titulo.append(soup.findAll('article')[i].find('h3').text.replace('\\r', '').replace('\\n', '').replace('\\t', ''))\n",
    "        except:\n",
    "            titulo.append('Revisar')\n",
    "\n",
    "        # Autor\n",
    "        por.append(None)\n",
    "\n",
    "        # Cabecera\n",
    "        cab.append(None)\n",
    "\n",
    "        # Comentarios\n",
    "        com.append(None)\n",
    "\n",
    "        # Página\n",
    "        pag.append(n+1)\n",
    "\n",
    "        # URL\n",
    "        urll = 'https://www.eljueves.es' + soup.findAll('article')[i].find('div',  {'class' : \"txt\"}).find('a').get('href')\n",
    "        url.append(urll)\n",
    "\n",
    "        # Texto\n",
    "        #texto.append('Revisar')\n",
    "        peticion_noticia = requests.get(urll, allow_redirects = False)\n",
    "        soup_noticia = BeautifulSoup(peticion_noticia.text, \"lxml\")\n",
    "\n",
    "        tex = []\n",
    "\n",
    "        try:\n",
    "            tex.append(soup_noticia.findAll('h2')[0].text.replace('\\r', '').replace('\\n', '').replace('\\t', ''))\n",
    "        except:\n",
    "            tex.append('')\n",
    "\n",
    "        try:\n",
    "            for j in range(0, len(soup_noticia.findAll('div')[2].findAll('p', {'class' : ''})[4:-1])):\n",
    "                tex.append(soup_noticia.findAll('div')[2].findAll('p', {'class' : ''})[j+4].text)\n",
    "            tex = ' '.join(tex)\n",
    "\n",
    "        except:\n",
    "            tex.append('Revisar')\n",
    "\n",
    "        texto.append(tex)\n",
    "\n",
    "    \n",
    "\n",
    "table_dict = {'Web': web,\\\n",
    "              'Título': titulo,\\\n",
    "              'Autor': por,\\\n",
    "              'Cabecera': cab,\\\n",
    "              'Comentarios': com,\\\n",
    "              'Texto': texto,\\\n",
    "              'Página' : pag,\\\n",
    "              'URL': url}\n",
    "\n",
    "ej = pd.DataFrame(table_dict) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataframe conjunto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>new_id</th>\n",
       "      <th>Web</th>\n",
       "      <th>Título</th>\n",
       "      <th>Autor</th>\n",
       "      <th>Cabecera</th>\n",
       "      <th>Comentarios</th>\n",
       "      <th>Texto</th>\n",
       "      <th>Página</th>\n",
       "      <th>URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>new_1</td>\n",
       "      <td>El Mundo Today</td>\n",
       "      <td>Como cada año, los informativos vuelven a envi...</td>\n",
       "      <td>Kike García</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>“6000 GRADOS, VICENTE, TEMPERATURAS ALTAS PERO...</td>\n",
       "      <td>1</td>\n",
       "      <td>https://www.elmundotoday.com/2020/07/como-cada...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>new_2</td>\n",
       "      <td>El Mundo Today</td>\n",
       "      <td>Tras veinte minutos intentando descifrar la le...</td>\n",
       "      <td>Xavi Puig</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>\"PUES NO SÉ, PÓNGASE UNA PELÍCULA DE ESTA CHIC...</td>\n",
       "      <td>1</td>\n",
       "      <td>https://www.elmundotoday.com/2020/07/tras-vein...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>new_3</td>\n",
       "      <td>El Mundo Today</td>\n",
       "      <td>Acusan a una astróloga de hacer pronósticos be...</td>\n",
       "      <td>Javi Ramos</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>LOS CAPRICORNIO LLEVAN AÑOS VIVIENDO MUCHO MEJ...</td>\n",
       "      <td>1</td>\n",
       "      <td>https://www.elmundotoday.com/2020/07/acusan-a-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>new_4</td>\n",
       "      <td>El Mundo Today</td>\n",
       "      <td>Hacienda informó reiteradamente a la Audiencia...</td>\n",
       "      <td>Kike García</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>NO EXISTE NINGÚN TRABAJO PROBADO POR EL QUE PU...</td>\n",
       "      <td>1</td>\n",
       "      <td>https://www.elmundotoday.com/2020/07/hacienda-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>new_5</td>\n",
       "      <td>El Mundo Today</td>\n",
       "      <td>La Asociación Española de Berenjenas denuncia ...</td>\n",
       "      <td>Kike García</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>“BASTA”, DICEN LAS BERENJENAS Tras creer duran...</td>\n",
       "      <td>1</td>\n",
       "      <td>https://www.elmundotoday.com/2020/07/la-asocia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>new_241</td>\n",
       "      <td>El Jueves</td>\n",
       "      <td>Miguel Bosé denuncia un complot para ocultar q...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>'La expresión mirando a Cuenca se utiliza porq...</td>\n",
       "      <td>2</td>\n",
       "      <td>https://www.eljueves.es/news/miguel-bose-denun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>new_242</td>\n",
       "      <td>El Jueves</td>\n",
       "      <td>El rey Juan Carlos también pagó una despedida ...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>\"No actué como rey. Solo como catador real\" ac...</td>\n",
       "      <td>2</td>\n",
       "      <td>https://www.eljueves.es/news/rey-juan-carlos-t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>new_243</td>\n",
       "      <td>El Jueves</td>\n",
       "      <td>El Apocalipsis maya se queda en casa después d...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>\"Si es que ya es ensañarse\" declara Hace unos ...</td>\n",
       "      <td>2</td>\n",
       "      <td>https://www.eljueves.es/news/apocalipsis-maya-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>new_244</td>\n",
       "      <td>El Jueves</td>\n",
       "      <td>Llevar la mascarilla en el codo por la calle p...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Aunque inicialmente fue una práctica desacons...</td>\n",
       "      <td>2</td>\n",
       "      <td>https://www.eljueves.es/news/llevar-mascarilla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>new_245</td>\n",
       "      <td>El Jueves</td>\n",
       "      <td>Ancianos se disfrazan para poder ser atendidos...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Las últimas noticias desvelando ciertas técni...</td>\n",
       "      <td>2</td>\n",
       "      <td>https://www.eljueves.es/news/ancianos-se-disfr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>245 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      new_id             Web  \\\n",
       "0      new_1  El Mundo Today   \n",
       "1      new_2  El Mundo Today   \n",
       "2      new_3  El Mundo Today   \n",
       "3      new_4  El Mundo Today   \n",
       "4      new_5  El Mundo Today   \n",
       "..       ...             ...   \n",
       "240  new_241       El Jueves   \n",
       "241  new_242       El Jueves   \n",
       "242  new_243       El Jueves   \n",
       "243  new_244       El Jueves   \n",
       "244  new_245       El Jueves   \n",
       "\n",
       "                                                Título        Autor Cabecera  \\\n",
       "0    Como cada año, los informativos vuelven a envi...  Kike García        1   \n",
       "1    Tras veinte minutos intentando descifrar la le...    Xavi Puig        0   \n",
       "2    Acusan a una astróloga de hacer pronósticos be...   Javi Ramos        0   \n",
       "3    Hacienda informó reiteradamente a la Audiencia...  Kike García        0   \n",
       "4    La Asociación Española de Berenjenas denuncia ...  Kike García        0   \n",
       "..                                                 ...          ...      ...   \n",
       "240  Miguel Bosé denuncia un complot para ocultar q...         None     None   \n",
       "241  El rey Juan Carlos también pagó una despedida ...         None     None   \n",
       "242  El Apocalipsis maya se queda en casa después d...         None     None   \n",
       "243  Llevar la mascarilla en el codo por la calle p...         None     None   \n",
       "244  Ancianos se disfrazan para poder ser atendidos...         None     None   \n",
       "\n",
       "    Comentarios                                              Texto Página  \\\n",
       "0          None  “6000 GRADOS, VICENTE, TEMPERATURAS ALTAS PERO...      1   \n",
       "1          None  \"PUES NO SÉ, PÓNGASE UNA PELÍCULA DE ESTA CHIC...      1   \n",
       "2          None  LOS CAPRICORNIO LLEVAN AÑOS VIVIENDO MUCHO MEJ...      1   \n",
       "3          None  NO EXISTE NINGÚN TRABAJO PROBADO POR EL QUE PU...      1   \n",
       "4          None  “BASTA”, DICEN LAS BERENJENAS Tras creer duran...      1   \n",
       "..          ...                                                ...    ...   \n",
       "240        None  'La expresión mirando a Cuenca se utiliza porq...      2   \n",
       "241        None  \"No actué como rey. Solo como catador real\" ac...      2   \n",
       "242        None  \"Si es que ya es ensañarse\" declara Hace unos ...      2   \n",
       "243        None   Aunque inicialmente fue una práctica desacons...      2   \n",
       "244        None   Las últimas noticias desvelando ciertas técni...      2   \n",
       "\n",
       "                                                   URL  \n",
       "0    https://www.elmundotoday.com/2020/07/como-cada...  \n",
       "1    https://www.elmundotoday.com/2020/07/tras-vein...  \n",
       "2    https://www.elmundotoday.com/2020/07/acusan-a-...  \n",
       "3    https://www.elmundotoday.com/2020/07/hacienda-...  \n",
       "4    https://www.elmundotoday.com/2020/07/la-asocia...  \n",
       "..                                                 ...  \n",
       "240  https://www.eljueves.es/news/miguel-bose-denun...  \n",
       "241  https://www.eljueves.es/news/rey-juan-carlos-t...  \n",
       "242  https://www.eljueves.es/news/apocalipsis-maya-...  \n",
       "243  https://www.eljueves.es/news/llevar-mascarilla...  \n",
       "244  https://www.eljueves.es/news/ancianos-se-disfr...  \n",
       "\n",
       "[245 rows x 9 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lista con los dataframes que tenemos de los diferentes periódicos\n",
    "fake_news = [emt, mb, nb, nf, ej]\n",
    "\n",
    "# Los unimos, y para que no se repitan los índices, los reseteamos\n",
    "df = pd.concat(fake_news).reset_index(drop=True)\n",
    "\n",
    "# Para crear el id (único) de cada noticia\n",
    "new_id = 'new_' + (df.index + 1).astype(str)\n",
    "\n",
    "# Y lo insertamos en la primera posición\n",
    "df.insert(loc=0, column='new_id', value=new_id)\n",
    "\n",
    "# Ver el dataframe final\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "245"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exportar a csv\n",
    "nombre = 'fake_news_' + datetime.today().strftime('%d%m%Y') + '.csv'\n",
    "df.to_csv(nombre, encoding='utf-8-sig', header = True, sep = ';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de nulos en titulares:  48\n"
     ]
    }
   ],
   "source": [
    "print('Cantidad de nulos en titulares: ', len(df[df['Título'].isnull()]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
