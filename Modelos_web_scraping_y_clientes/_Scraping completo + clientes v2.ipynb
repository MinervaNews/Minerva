{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web scraping completo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si se ejecuta muchas veces seguidas da error por \"exceso de redirecciones\". Para solucionarlo hay que limpiar caché y borrar las cookies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Periódicos incluidos: \n",
    "- El Confindencial (general)\n",
    "- El Mundo (general)\n",
    "- El País (general)\n",
    "- ABC (general)\n",
    "- El Economista (económico)\n",
    "- Marca (deportivo)\n",
    "- La Razón (general)\n",
    "- Cinco Días (económico)\n",
    "- As (deportivo)\n",
    "- Expansión (económico)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import copy\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## El Confidencial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# URL de la que partimos\n",
    "url = 'https://www.elconfidencial.com/espana/'\n",
    "peticion = requests.get(url, allow_redirects = False)\n",
    "soup = BeautifulSoup(peticion.text, \"lxml\")\n",
    "\n",
    "\n",
    "# Comprobar cuál es la última página de noticias (como tiene varios números en el pie de página para seguir avanzando...)\n",
    "last_page = int(soup.findAll('a',{'rel' : 'last'})[0]\\\n",
    "                .get('href')[:-1][soup.findAll('a',{'rel' : 'last'})[0].get('href')[:-1].rfind('/')+1:])\n",
    "\n",
    "\n",
    "# Hacer lista con todas las url de cada página de noticias (elconfidencial/espana/2, elconfidencial/espana/3, etc.)\n",
    "list_urls = [url] # la inicializamos con la propia home (elconfidencial/espana)\n",
    "for i in range(2, last_page+1): # range 2 porque la 1 es la home\n",
    "    list_urls.append(url + str(i) + '/')\n",
    "   \n",
    "# En ocasiones se generan errores al hacer scraping sobre todas las páginas de El Confidencial. Si es así, hay que \n",
    "# descomentar la siguiente línea y sacar solo las noticias de la primera página:\n",
    "\n",
    "#list_urls = [url]\n",
    "\n",
    "# Inicializamos listas vacías para llenarlas con bucles\n",
    "web = [] # será siempre \"El Confidencial\", para diferenciar este dataset de otros\n",
    "titulo = [] # para el título de la noticia\n",
    "por = [] # para el autor \n",
    "cab = [] # para indicar si el artículo está en la cabecera o no. Binario (1 = sí está en cabecera, 0 = no)\n",
    "com = [] # para el número de comentarios que tiene\n",
    "texto = [] # para el texto de la noticia\n",
    "pag = [] # para el número de página en el que está la noticia (entiendo que las que están en la pg 1 son más \"importantes\"\n",
    "         #                                                     que las de la página 12, o al menos más recientes)\n",
    "url = [] # para la url de la noticia, que sería la específica que el sistema le recomendaría (o no) al usuario\n",
    "\n",
    "\n",
    "# Bucle para iterar sobre cada una de las páginas de noticias\n",
    "for x, urll in enumerate(list_urls):\n",
    "    i = 0\n",
    "    peticion = requests.get(urll, allow_redirects = False)\n",
    "    soup = BeautifulSoup(peticion.text, \"lxml\")\n",
    "\n",
    "    # Para la noticia que esté en la cabecera - no todas tienen, por eso ponemos esta parte en \"try\"\n",
    "    try:\n",
    "        titulo.append(soup.findAll('a', attrs={'class': 'archive-article-top-link'})[0].get('data-title'))\n",
    "        por.append(soup.findAll('span',{'class' : 'archive-article-top-author sig-color'})[0].text)\n",
    "        cab.append(1) # 1 porque es la de la cabecera\n",
    "        # Para comentarios: si no tiene, muestra 0. Si sí, muestra el número de comentarios\n",
    "        if soup.findAll('div',{'class' : 'archive-header-top-author-box'})[0].find('em') == None:\n",
    "            com.append(0)\n",
    "        else:\n",
    "            com.append(soup.findAll('div',{'class' : 'archive-header-top-author-box'})[0].find('em').text)\n",
    "        \n",
    "        pag.append(x+1) # x se saca del inicio del bucle, por \"enumerate\", como comienza en 0, se le suma 1 al índice\n",
    "        \n",
    "        # Para la url de cada noticia\n",
    "        url_noticia = soup.findAll('a', attrs={'class': 'archive-article-top-link'})[0].get('href')\n",
    "        url.append(url_noticia)\n",
    "        peticion_noticia = requests.get(url_noticia, allow_redirects = False)\n",
    "        soup_noticia = BeautifulSoup(peticion_noticia.text, \"lxml\")\n",
    "        texto.append(soup_noticia.findAll('div', attrs={'class': 'news-body-center cms-format'})[0].text)\n",
    "        \n",
    "        web.append('El Confidencial')\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # Para el resto de noticias, las que no están en cabecera\n",
    "    # Bucle desde 0 hasta la cantidad total de noticias que haya en cada una de las páginas\n",
    "    \n",
    "    for i in range(0,len(soup.findAll('a', attrs={'href': re.compile(\"^https://\"), 'class': 'archive-article-link'}))):\n",
    "        web.append('El Confidencial')\n",
    "        titulo.append(soup.findAll('a', attrs={'href': re.compile(\"^https://\"), 'class': 'archive-article-link'})[i].get('title'))\n",
    "        por.append(soup.findAll('span',{'class' : 'archive-article-author sig-color'})[i].text)\n",
    "        cab.append(0) # 0 porque no son noticias de cabecera\n",
    "        # Para comentarios: si no tiene, muestra 0. Si sí, muestra el número de comentarios\n",
    "        if soup.findAll('div',{'class' : 'archive-article-author-box'})[i].find('em') == None:\n",
    "            com.append(0)\n",
    "        else:\n",
    "            com.append(soup.findAll('div',{'class' : 'archive-article-author-box'})[i].find('em').text)\n",
    "        pag.append(x+1)\n",
    "        \n",
    "        # Para la url de cada noticia\n",
    "        url_noticia = soup.findAll('a', attrs={'href': re.compile(\"^https://\"), 'class': 'archive-article-link'})[i].get('href')\n",
    "        url.append(url_noticia)\n",
    "        peticion_noticia = requests.get(url_noticia, allow_redirects = False)\n",
    "        soup_noticia = BeautifulSoup(peticion_noticia.text, \"lxml\")\n",
    "        \n",
    "        # Como el cuerpo de la noticia está en formatos diferentes en función de cada noticia:\n",
    "        try:\n",
    "            texto.append(soup_noticia.findAll('div', attrs={'class': 'news-body-center cms-format'})[0].text)\n",
    "        except:\n",
    "            try:\n",
    "                texto.append(soup_noticia.findAll('div', attrs={'class': 'newsType__content'})[0].text)\n",
    "            except:\n",
    "                texto.append('Revisar') # hay noticias que no tienen texto, son solo vídeos o cosas interactivas\n",
    "\n",
    "# Hacemos un diccionario con todos los resultados obtenidos y lo pasamos a formato dataframe de Pandas\n",
    "\n",
    "table_dict = {'Web': web,\\\n",
    "              'Título': titulo,\\\n",
    "              'Autor': por,\\\n",
    "              'Cabecera': cab,\\\n",
    "              'Comentarios': com,\\\n",
    "              'Texto': texto,\\\n",
    "              'Página' : pag,\\\n",
    "              'URL': url}\n",
    "\n",
    "ec = pd.DataFrame(table_dict) # \"ec\" = El Confidencial\n",
    "\n",
    "# Para sacar la fecha de la noticia desde la url de la misma (yyyy-mm-dd):\n",
    "ec['fecha1'] = ec['URL'].str.split('/').str[4] # divide la url por cada \"/\" que encuentre y nos quedamos con el de la posición 4\n",
    "ec['fecha2'] = ec['URL'].str.split('/').str[5] # divide la url por cada \"/\" que encuentre y nos quedamos con el de la posición 5\n",
    "ec['fecha3'] = ec['URL'].str.split('/').str[6] # divide la url por cada \"/\" que encuentre y nos quedamos con el de la posición 6\n",
    "ec['fecha4'] = ec['fecha1'].apply(lambda x: len([s for s in x if s.isdigit()])) # cuenta los números que hay en este campo\n",
    "ec['fecha5'] = ec['fecha2'].apply(lambda x: len([s for s in x if s.isdigit()]))\n",
    "ec['fecha6'] = ec['fecha3'].apply(lambda x: len([s for s in x if s.isdigit()]))\n",
    "\n",
    "# Función personalizada para sacar cuál es la fecha\n",
    "def fecha(row):\n",
    "    if row['fecha4'] == 8: # para que sea una fecha, debe tener 8 números (yyyy-mm-dd)\n",
    "        return row['fecha1']\n",
    "    if row['fecha5'] == 8: # si el primer campo no es una fecha, prueba con el segundo\n",
    "        return row['fecha2']\n",
    "    if row['fecha6'] == 8: # si el segundo campo no es una fecha, prueba con el tercero\n",
    "        return row['fecha3']\n",
    "    else:\n",
    "        return 'Otra'\n",
    "    \n",
    "ec['Fecha'] = ec.apply (lambda row: fecha(row), axis=1) # aplicamos la función anterior a la columna \"Fecha\" para sacar la \n",
    "                                                        # fecha definitiva\n",
    "\n",
    "# Borramos las columnas auxiliares\n",
    "del ec['fecha1']\n",
    "del ec['fecha2']\n",
    "del ec['fecha3']\n",
    "del ec['fecha4']\n",
    "del ec['fecha5']\n",
    "del ec['fecha6']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## El Mundo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL de la que partimos\n",
    "url = 'https://www.elmundo.es/espana.html'\n",
    "peticion = requests.get(url)\n",
    "soup = BeautifulSoup(peticion.text, \"lxml\")\n",
    "\n",
    "# Inicializamos listas vacías para llenarlas con bucles\n",
    "web = [] # será siempre \"El Mundo\", para diferenciar este dataset de otros\n",
    "titulo = [] # para el título de la noticia\n",
    "por = [] # para el autor \n",
    "cab = [] # indicador de cabecer\n",
    "aux = [] # auxiliar para los comentarios\n",
    "com = [] # para el número de comentarios que tiene\n",
    "texto = [] # para el texto de la noticia\n",
    "pag = [] # número de página en el que está la noticia\n",
    "url = [] # para la url de la noticia\n",
    "\n",
    "# Para sacar los autores\n",
    "nested_lst = soup.findAll('div', {\"class\" : 'ue-c-cover-content__list-inline'})\n",
    "lista = []\n",
    "for i in range(0, len(nested_lst)):\n",
    "    lista.append(nested_lst[i].text)\n",
    "\n",
    "for l in range(0, len(lista)):\n",
    "    result = re.search('\\n(.*)\\n', lista[l])\n",
    "    if result is None: \n",
    "        por.append('-')\n",
    "    else:\n",
    "        por.append(result.group(1))\n",
    "\n",
    "# Para sacar los comentarios  \n",
    "for i in range(0, len(lista)):\n",
    "    aux.append(lista[i].rsplit('\\n', 1)[-1])\n",
    "\n",
    "for c in range(0, len(aux)):\n",
    "    prev = aux[c].split(\"}})\")\n",
    "    if len(prev) == 1:\n",
    "        com.append(prev[0].split(' ')[0])\n",
    "    else:\n",
    "        com.append(prev[-1].split(' ')[0])\n",
    "\n",
    "# Resto: título, cabecera siempre 0, página siempre 1\n",
    "for i in range(0,len(soup.findAll('h2', {\"class\" : 'ue-c-cover-content__headline'}))):\n",
    "    web.append('El Mundo')\n",
    "    \n",
    "    try:\n",
    "        titulo.append(\n",
    "            soup.findAll('div', {\"class\" : 'ue-c-cover-content__body'})[i].find('span', {'class' : 'ue-c-cover-content__kicker'}).text\\\n",
    "                  + soup.findAll('h2', {\"class\" : 'ue-c-cover-content__headline'})[i].text)\n",
    "    except:\n",
    "        titulo.append(soup.findAll('h2', {\"class\" : 'ue-c-cover-content__headline'})[i].text)\n",
    "        \n",
    "    # Cabecera\n",
    "    if i == 0:\n",
    "        cab.append(1) \n",
    "    else:\n",
    "        cab.append(0)\n",
    "    \n",
    "    pag.append(1) # Está todo en la misma página, no hay varias\n",
    "    \n",
    "    # Para la url\n",
    "    url_noticia = soup.findAll('a', attrs={'href': re.compile(\"^https://\"), 'class': 'ue-c-cover-content__link'})[i].get('href')\n",
    "    url.append(url_noticia)\n",
    "    peticion_noticia = requests.get(url_noticia, allow_redirects = False)\n",
    "    soup_noticia = BeautifulSoup(peticion_noticia.text, \"lxml\")\n",
    "    \n",
    "    \n",
    "    # Para el texto de la noticia. Como hay diferentes formatos, los meto en distintos \"try\"\n",
    "    # EL MUNDO ES DE PAGO, POR LO QUE SOLO EXTRAE LA PARTE GRATUITA DE LA NOTICIA (1 párrafo, generalmente)\n",
    "    try:\n",
    "        #texto.append(soup_noticia.findAll('p', {'class': 'ue-c-article--first-letter-highlighted'})[0].text)\n",
    "        raw = []\n",
    "        for elm in soup_noticia.select('p'):\n",
    "            if elm.find('span'):\n",
    "                    continue\n",
    "            raw.append(elm)\n",
    "\n",
    "        raw2 = str(raw).split('<p class=\"ue-c-article__standfirst\">')[1].split('<p class=\"ue-c-newsletter-widget__subline\">')[0]\n",
    "        texto.append(re.sub('<[^>]+>', '', raw2))\n",
    "        \n",
    "    except:\n",
    "        try:\n",
    "            texto.append(soup_noticia.findAll('section', {'class': 'cuerpo'})[0].text)\n",
    "        except:\n",
    "            try:\n",
    "                texto.append(soup_noticia.findAll('p', {'class': 'ue-c-article--first-letter-highlighted'})[0].text)\n",
    "            except:\n",
    "                try:\n",
    "                    tex = []\n",
    "\n",
    "                    for i in range(0, len(soup.findAll('p', {'class': ''}))):\n",
    "                        tex.append(soup.findAll('p', {'class': ''})[i].text)\n",
    "\n",
    "                    tex2 = ' '.join(tex)\n",
    "                    texto.append(tex2)\n",
    "                except:\n",
    "                    try:\n",
    "                        texto.append(soup_noticia.findAll('dl')[0].text)\n",
    "                    except:\n",
    "                        texto.append(None)\n",
    "            \n",
    "\n",
    "    \n",
    "# Hacemos un diccionario con todos los resultados obtenidos y lo pasamos a formato dataframe\n",
    "table_dict = {'Web': web,\\\n",
    "              'Título': titulo,\\\n",
    "              'Autor': por,\\\n",
    "              'Cabecera': cab,\\\n",
    "              'Comentarios': com,\\\n",
    "              'Texto': texto,\\\n",
    "              'Página' : pag,\\\n",
    "              'URL': url}\n",
    "em = pd.DataFrame(table_dict)\n",
    "\n",
    "# Para sacar la fecha desde la url (yyyy-mm-dd)\n",
    "em['Fecha'] = em['URL'].str.split('/').str[4] + '-' + em['URL'].str.split('/').str[5] + '-' + em['URL'].str.split('/').str[6]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## El País"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No se puede sacar el número de comentarios con BeautifulSoup, se podría sacar con Selenium/webdriver en caso de que fuese necesario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL de la que partimos\n",
    "url = 'https://elpais.com'\n",
    "peticion = requests.get(url)\n",
    "soup = BeautifulSoup(peticion.text, \"lxml\")\n",
    "\n",
    "# Inicializamos listas vacías para llenarlas con bucles\n",
    "web = [] # será siempre \"El País\"\n",
    "titulo = [] # para el título de la noticia\n",
    "por = [] # para el autor \n",
    "cab = [] # indicador de cabecera\n",
    "com = [] # para el número de comentarios que tiene\n",
    "texto = [] # para el texto de la noticia\n",
    "pag = [] # número de página en el que está la noticia\n",
    "url = [] # para la url de la noticia\n",
    "\n",
    "for i in range(0, len(soup.findAll('article'))):\n",
    "    web.append('El País') # Siempre el mismo\n",
    "    #titulo.append(soup.findAll('article')[i].find('h2').text)\n",
    "    tit = soup.findAll('article')[i].find('h2').text\n",
    "    titulo.append(tit)\n",
    "    \n",
    "    # Autor\n",
    "    try:\n",
    "        aut = []\n",
    "        for j in range(0, len(soup.findAll('article')[i].findAll('a', {'class' : 'author'}))):\n",
    "            aut.append(soup.findAll('article')[i].findAll('a', {'class' : 'author'})[j].text)\n",
    "\n",
    "        autt = list(aut)\n",
    "        por.append(' '.join(autt))\n",
    "    except:\n",
    "        por.append('Revisar')\n",
    "    \n",
    "    # Cabecera: 1 si es la primera noticia, 0 el resto\n",
    "    if i == 0:\n",
    "        cab.append(1)\n",
    "    else: \n",
    "        cab.append(0)\n",
    "     \n",
    "    com.append(None) # No se puede sacar con BeautifulSoup. Se podría con Selenium / Webdriver\n",
    "    pag.append(1) # Están todas las noticias en la home\n",
    "    \n",
    "    # URL\n",
    "    \n",
    "    try:\n",
    "        prev = soup.findAll('article')[i].findAll('a', {'class' : False})\n",
    "        if 'por=mosaico' in prev[0].get('href'):\n",
    "            url_aux = prev[1].get('href')\n",
    "        else:\n",
    "            url_aux = prev[0].get('href')\n",
    "\n",
    "        if url_aux[0] == '/':\n",
    "            urll = 'https://elpais.com' + url_aux\n",
    "        else:\n",
    "            urll = url_aux\n",
    "\n",
    "        url.append(urll)\n",
    "    except:\n",
    "        urll = None\n",
    "        url.append(urll)\n",
    "    \n",
    "    try:\n",
    "        if 'huffingtonpost' not in urll: # la web de Huffingtonpost da errores \n",
    "            # Texto\n",
    "            peticion_noticia = requests.get(urll, allow_redirects = False)\n",
    "            soup_noticia = BeautifulSoup(peticion_noticia.text, \"lxml\")\n",
    "\n",
    "            # Como el cuerpo de la noticia está en formatos diferentes en función de cada noticia:\n",
    "            try:\n",
    "                texto.append(soup_noticia.findAll('div', attrs={'class': 'a_b article_body | color_gray_dark'})[0].text)\n",
    "            except:\n",
    "                texto.append(None) # hay noticias que no tienen texto, son solo vídeos o cosas interactivas\n",
    "\n",
    "        else:\n",
    "            #texto.append('Hufftington')\n",
    "            texto.append(tit)\n",
    "    except:\n",
    "        texto.append(None)\n",
    "    \n",
    "# Hacemos un diccionario con todos los resultados obtenidos y lo pasamos a formato dataframe\n",
    "table_dict = {'Web': web,\\\n",
    "              'Título': titulo,\\\n",
    "              'Autor': por,\\\n",
    "              'Cabecera': cab,\\\n",
    "              'Comentarios': com,\\\n",
    "              'Texto': texto,\\\n",
    "              'Página' : pag,\\\n",
    "              'URL': url}\n",
    "ep = pd.DataFrame(table_dict)\n",
    "\n",
    "\n",
    "# Para sacar la fecha de la noticia desde la url de la misma (yyyy-mm-dd):\n",
    "r = re.compile(\"\\d{4}-\\d{2}-\\d{2}\")\n",
    "r2 = re.compile(\"\\d{4}/\\d{2}/\\d{2}\")\n",
    "\n",
    "def match(x):\n",
    "    m = r.search(x)\n",
    "    m2 = r2.search(x)\n",
    "    if m:\n",
    "        return  m.group()\n",
    "    elif m2:\n",
    "        return m2.group()\n",
    "    return  \"\"\n",
    "\n",
    "try:\n",
    "    ep['Fecha'] = ep[\"URL\"].apply(match) # aplicamos la función anterior a la columna \"URL\" para sacar la \n",
    "                                                        # fecha definitiva\n",
    "    ep['Fecha'] = ep['Fecha'].str.replace('/','-')\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ABC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL de la que partimos\n",
    "url = 'https://www.abc.es/'\n",
    "peticion = requests.get(url)\n",
    "soup = BeautifulSoup(peticion.text, \"lxml\")\n",
    "\n",
    "# Inicializamos listas vacías para llenarlas con bucles\n",
    "web = [] # será siempre \"ABC\"\n",
    "titulo = [] # para el título de la noticia\n",
    "por = [] # para el autor \n",
    "cab = [] # indicador de cabecera\n",
    "com = [] # para el número de comentarios que tiene\n",
    "texto = [] # para el texto de la noticia\n",
    "pag = [] # número de página en el que está la noticia\n",
    "url = [] # para la url de la noticia\n",
    "\n",
    "for i in range(0, len(soup.findAll('span', {'class' : 'textos'}))):\n",
    "    web.append('ABC')\n",
    "    \n",
    "    # Titular\n",
    "    try:\n",
    "        titulo.append(soup.findAll('span', {'class' : 'textos'})[i].find('a').get('title'))\n",
    "    except:\n",
    "        titulo.append('Revisar')\n",
    "    \n",
    "    # Autor\n",
    "    try:\n",
    "        aux = soup.findAll('span', {'class' : 'textos'})[i].find('footer').find('a', {'class' : 'autor'}).text\n",
    "    except:\n",
    "        try:\n",
    "            aux = soup.findAll('span', {'class' : 'textos'})[i].find('footer').find('span').text\n",
    "        except:\n",
    "            aux = 'Revisar'\n",
    "    if aux == 'Comentar':\n",
    "        por.append(None)\n",
    "    else:\n",
    "        por.append(aux)\n",
    "\n",
    "    # Cabecera\n",
    "    if i == 0:\n",
    "        cab.append(1)\n",
    "    else:\n",
    "        cab.append(0)\n",
    "    \n",
    "    \n",
    "    com.append(None) # Incapaz de sacarlo con BeautifulSoup\n",
    "    pag.append(1)\n",
    "    \n",
    "    # URL\n",
    "    try:\n",
    "        urll = soup.findAll('span', {'class' : 'textos'})[i].find('a').get('href')\n",
    "        if urll[0] == '/':\n",
    "            url2 = 'https://www.abc.es' + urll\n",
    "        elif urll[0] == 't':\n",
    "            url2 = 'h' + urll\n",
    "        else:    \n",
    "            url2 = urll\n",
    "        url.append(url2)\n",
    "    except:\n",
    "        url.append('Revisar')\n",
    "    \n",
    "    # Texto\n",
    "    peticion_noticia = requests.get(url2, allow_redirects = False)\n",
    "    soup_noticia = BeautifulSoup(peticion_noticia.text, \"lxml\")\n",
    "    \n",
    "    tex = []\n",
    "    for i in range(0, len(soup_noticia.findAll('p', {'class' : ''})[1:])):\n",
    "        tex.append(soup_noticia.findAll('p', {'class' : ''})[1:][i].text)\n",
    "    texto.append(' '.join(tex))\n",
    "\n",
    "    \n",
    "    \n",
    "# Hacemos un diccionario con todos los resultados obtenidos y lo pasamos a formato dataframe\n",
    "table_dict = {'Web': web,\\\n",
    "              'Título': titulo,\\\n",
    "              'Autor': por,\\\n",
    "              'Cabecera': cab,\\\n",
    "              'Comentarios': com,\\\n",
    "              'Texto': texto,\\\n",
    "              'Página' : pag,\\\n",
    "              'URL': url}\n",
    "abc = pd.DataFrame(table_dict)\n",
    "\n",
    "\n",
    "# Fecha\n",
    "r = re.compile(\"\\d{4}\\d{2}\\d{2}\")\n",
    "\n",
    "def match(x):\n",
    "    m = r.search(x)\n",
    "    if m:\n",
    "        return  m.group()\n",
    "    return  \"\"\n",
    "\n",
    "abc['Fecha'] = abc[\"URL\"].apply(match) # aplicamos la función anterior a la columna \"URL\" \n",
    "\n",
    "abc['Fecha'] = abc['Fecha'].apply(lambda s: \"{}-{}-{}\".format(s[0:4], s[4:6], s[6:]))\n",
    "abc['Fecha'] = abc['Fecha'].replace('//', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## El Economista"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL de la que partimos\n",
    "url = 'https://www.eleconomista.es/'\n",
    "peticion = requests.get(url, allow_redirects = False)\n",
    "soup = BeautifulSoup(peticion.text, \"lxml\")\n",
    "\n",
    "# Inicializamos listas vacías para llenarlas con bucles\n",
    "web = [] # será siempre \"El Economista\"\n",
    "titulo = [] # para el título de la noticia\n",
    "por = [] # para el autor \n",
    "cab = [] # para indicar si el artículo está en la cabecera o no. Binario (1 = sí está en cabecera, 0 = no)\n",
    "com = [] # para el número de comentarios que tiene\n",
    "texto = [] # para el texto de la noticia\n",
    "pag = [] # para el número de página \n",
    "url = [] # para la url de la noticia, que sería la específica que el sistema le recomendaría (o no) al usuario\n",
    "fecha = []\n",
    "\n",
    "for i in range(0, len(soup.findAll('div', {'class' : 'article'}))):\n",
    "    web.append('El Economista')\n",
    "    \n",
    "    # Titular\n",
    "    try:\n",
    "        titulo.append(soup.findAll('div', {'class' : 'article'})[i].find('h2').text)\n",
    "    except:\n",
    "        titulo.append(soup.findAll('div', {'class' : 'article'})[i].find('span').text)\n",
    "    \n",
    "    # Cabecera = 1 si es la primera noticia, 0 para el resto\n",
    "    if i == 0:\n",
    "        cab.append(1)\n",
    "    else:\n",
    "        cab.append(0)\n",
    "    \n",
    "    # Página = 1 siempre\n",
    "    pag.append(1)\n",
    "    \n",
    "    # URL\n",
    "    try:\n",
    "        urll = soup.findAll('div', {'class' : 'article'})[i].find('h2').find('a').get('href')\n",
    "        if urll[0] == '/':\n",
    "            urll = 'https:' + urll\n",
    "        url.append(urll)\n",
    "    except:\n",
    "        try:\n",
    "            urll = soup.findAll('div', {'class' : 'article'})[i].find('span').find('a').get('href')\n",
    "            if urll[0] == '/':\n",
    "                urll = 'https:' + urll\n",
    "            url.append(urll)\n",
    "        except:\n",
    "            try:\n",
    "                urll = soup.findAll('div', {'class' : 'article'})[i].find('a').get('href')\n",
    "                if urll[0] == '/':\n",
    "                    urll = 'https:' + urll\n",
    "                url.append(urll)\n",
    "            except:\n",
    "                urll = 'Revisar'\n",
    "                url.append(urll)\n",
    "    \n",
    "    # Texto\n",
    "    #texto.append('Revisar')\n",
    "    try:\n",
    "        peticion_noticia = requests.get(urll, allow_redirects = False)\n",
    "        soup_noticia = BeautifulSoup(peticion_noticia.text, \"lxml\")\n",
    "        tex = soup_noticia.findAll('meta', property = 'og:description')[0]['content']\n",
    "        texto.append(tex)\n",
    "    except:\n",
    "        texto.append('Revisar')\n",
    "    \n",
    "    # Autor\n",
    "    try:\n",
    "        por.append(soup.findAll('div', {'class' : 'article'})[i].find('li', {'class' : 'articleAuthor'}).text)\n",
    "    except:\n",
    "        try:\n",
    "            aut = []\n",
    "            for a in range(0, len(soup_noticia.findAll('meta', itemprop = 'author'))):\n",
    "                aut.append(soup_noticia.findAll('meta', itemprop = 'author')[a]['content'])\n",
    "            aut = list(set(aut))\n",
    "            por.append(aut)\n",
    "        except:\n",
    "            por.append('Revisar')\n",
    "    \n",
    "    # Comentarios - formato JSON\n",
    "    try:\n",
    "        com.append(soup.findAll('div', {'class' : 'article'})[i].find('ul').findAll('li', {'class' : 'articleComments'})[0].find('span').text)\n",
    "    except:\n",
    "        try:\n",
    "            json_data = soup_noticia.findAll('script', text=re.compile(\"noticiaComentarios\"))\n",
    "            json_str = str(json_data).split(\"noticiaComentarios\")[1].replace(\"'\", \"\")\n",
    "            json_com = [int(s) for s in json_str.split() if s.isdigit()][0]\n",
    "            com.append(json_com)\n",
    "        except:\n",
    "            com.append(None)\n",
    "    \n",
    "    # Fecha \n",
    "    try:\n",
    "        json_data = soup_noticia.findAll('script', text=re.compile(\"noticiaComentarios\"))\n",
    "        json_str = str(json_data).split('noticiaFechaPublicacion')[1].split('noticiaComentarios')[0].replace(\"'\", \"\").replace(',','')\n",
    "        aux = [int(s) for s in json_str.split() if s.isdigit()]\n",
    "        fecha.append(str(datetime.date(datetime.strptime(str(aux[0])[0:8], '%Y%m%d'))))\n",
    "    except:\n",
    "        fecha.append(None)\n",
    "        \n",
    "# Hacemos un diccionario con todos los resultados obtenidos y lo pasamos a formato dataframe de Pandas\n",
    "\n",
    "table_dict = {'Web': web,\\\n",
    "              'Título': titulo,\\\n",
    "              'Autor': por,\\\n",
    "              'Cabecera': cab,\\\n",
    "              'Comentarios': com,\\\n",
    "              'Texto': texto,\\\n",
    "              'Página' : pag,\\\n",
    "              'URL': url,\\\n",
    "              'Fecha': fecha}\n",
    "\n",
    "ee = pd.DataFrame(table_dict) \n",
    "ee['Comentarios'] = ee['Comentarios'].replace('', 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Marca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.marca.com/'\n",
    "peticion = requests.get(url)\n",
    "soup = BeautifulSoup(peticion.text, \"lxml\")\n",
    "\n",
    "# Inicializamos listas vacías para llenarlas con bucles\n",
    "web = [] # será siempre \"Marca\"\n",
    "titulo = [] # para el título de la noticia\n",
    "por = [] # para el autor \n",
    "cab = [] # indicador de cabecera\n",
    "com = [] # para el número de comentarios que tiene\n",
    "texto = [] # para el texto de la noticia\n",
    "pag = [] # número de página en el que está la noticia\n",
    "url = [] # para la url de la noticia\n",
    "fecha = []\n",
    "\n",
    "for i in range(0, len(soup.findAll('article'))):\n",
    "    web.append('Marca')\n",
    "        \n",
    "    # Cabecera\n",
    "    if i == 0:\n",
    "        cab.append(1)\n",
    "    else:\n",
    "        cab.append(0)\n",
    "        \n",
    "    # Comentarios\n",
    "    try:\n",
    "        com.append(soup.findAll('article')[i].findAll('span', {'class' : 'number-comments'})[0].text)\n",
    "    except:\n",
    "        com.append(None)\n",
    "    \n",
    "    # Página\n",
    "    pag.append(1)\n",
    "    \n",
    "    # URL\n",
    "    try:\n",
    "        #urll = soup.findAll('article')[i].findAll('a', {'itemprop' : 'url'})[0].get('href')\n",
    "        urll = soup.findAll('article')[i].findAll('h2')[0].find('a').get('href')\n",
    "        url.append(urll)\n",
    "    except:\n",
    "        urll = None\n",
    "        url.append(None)\n",
    "    \n",
    "    # Texto\n",
    "    try:\n",
    "        peticion_noticia = requests.get(urll, allow_redirects = False)\n",
    "        soup_noticia = BeautifulSoup(peticion_noticia.text, \"lxml\")\n",
    "\n",
    "        tex = []\n",
    "        for j in range(0, len(soup_noticia.findAll('div', {'class' : 'row content cols-30-70'})[0].findAll('p', {'class' : ''}))):\n",
    "            tex.append(soup_noticia.findAll('div', {'class' : 'row content cols-30-70'})[0].findAll('p', {'class' : ''})[j].text)\n",
    "        texto.append(' '.join(tex))\n",
    "    except:\n",
    "        texto.append(None)\n",
    "\n",
    "     # Autores\n",
    "    try:\n",
    "        por.append(soup.findAll('article')[i].findAll('ul', {'class' : 'mod-author'})[0].find('span').text)\n",
    "    except:\n",
    "        try:\n",
    "            por.append(soup_noticia.findAll('ul', {'class' : 'author'})[0].text)\n",
    "        except:\n",
    "            por.append(None)\n",
    "    \n",
    "    # Titular\n",
    "    try:\n",
    "        #titulo.append(soup.findAll('article')[i].findAll('a', attrs={'itemprop': 'url'})[0].text)\n",
    "        titulo.append(soup.findAll('article')[i].findAll('h2')[0].text)\n",
    "    except:\n",
    "        try:\n",
    "            titulo.append(soup_noticia.findAll('div', {'class' : 'titles'})[0].findAll('h1')[0].text)\n",
    "        except:\n",
    "            titulo.append(None)\n",
    "    \n",
    "    # Fecha\n",
    "    # Para sacar la fecha de la noticia desde la url de la misma (yyyy-mm-dd):\n",
    "    r = re.compile(\"\\d{4}/\\d{2}/\\d{2}\")\n",
    "    r2 = re.compile(\"\\d{4}-\\d{2}-\\d{2}\")\n",
    "\n",
    "    def match(x):\n",
    "        m = r.search(x)\n",
    "        m2 = r2.search(x)\n",
    "        if m:\n",
    "            return  m.group()\n",
    "        elif m2:\n",
    "            return m2.group()\n",
    "        return  \"\"\n",
    "\n",
    "    try:\n",
    "        aux = match(urll).replace('/','-')\n",
    "        fecha.append(aux)\n",
    "    except:\n",
    "        fecha.append(None)\n",
    "    \n",
    "# Hacemos un diccionario con todos los resultados obtenidos y lo pasamos a formato dataframe\n",
    "table_dict = {'Web': web,\\\n",
    "              'Título': titulo,\\\n",
    "              'Autor': por,\\\n",
    "              'Cabecera': cab,\\\n",
    "              'Comentarios': com,\\\n",
    "              'Texto': texto,\\\n",
    "              'Página' : pag,\\\n",
    "              'URL': url,\\\n",
    "              'Fecha': fecha}\n",
    "\n",
    "m = pd.DataFrame(table_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## La Razón"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este diario contiene la información en formato JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL de la que partimos\n",
    "url = 'https://www.larazon.es/'\n",
    "peticion = requests.get(url)\n",
    "soup = BeautifulSoup(peticion.text, \"lxml\")\n",
    "\n",
    "# Inicializamos listas vacías para llenarlas con bucles\n",
    "web = [] # será siempre \"La Razón\"\n",
    "titulo = [] # para el título de la noticia\n",
    "por = [] # para el autor \n",
    "cab = [] # indicador de cabecera\n",
    "com = [] # para el número de comentarios que tiene\n",
    "texto = [] # para el texto de la noticia\n",
    "pag = [] # número de página en el que está la noticia\n",
    "url = [] # para la url de la noticia\n",
    "fecha = []\n",
    "\n",
    "counter = 0 # auxiliar para detectar cuál es la primera noticia (y así asignarla a cabecera)\n",
    "\n",
    "for i in range(0, len(soup.select(\"[type='application/ld+json']\"))):\n",
    "   \n",
    "    data = soup.select(\"[type='application/ld+json']\")[i]\n",
    "    \n",
    "    if json.loads(data.text) != []:\n",
    "        \n",
    "        counter += 1\n",
    "        web.append('La Razón')\n",
    "        \n",
    "        # Titular\n",
    "        json_headline = json.loads(data.text)['headline']\n",
    "        titulo.append(json_headline)\n",
    "        \n",
    "        # Fecha\n",
    "        json_date = json.loads(data.text)['datePublished'][:10]\n",
    "        fecha.append(json_date)\n",
    "        \n",
    "        # Autores\n",
    "        autores = []\n",
    "        for j in range(0, len(json.loads(data.text)['author'][0])):\n",
    "            autores.append(json.loads(data.text)['author'][0][j]['name'])\n",
    "        por.append(', '.join(autores))\n",
    "        \n",
    "        # Comentarios: no hay sección de comentarios de cada noticia, asignamos valor nulo\n",
    "        com.append(None)\n",
    "        \n",
    "        # Página siempre 1, están todas las noticias en la home\n",
    "        pag.append(1)\n",
    "        \n",
    "        # Cabecera\n",
    "        if counter == 1:\n",
    "            cab.append(1)\n",
    "        else:\n",
    "            cab.append(0)\n",
    "        \n",
    "        # URL\n",
    "        json_url = json.loads(data.text)['url']\n",
    "        url.append(json_url)\n",
    "        \n",
    "        # Texto\n",
    "        peticion_noticia = requests.get(json_url)\n",
    "        soup_noticia = BeautifulSoup(peticion_noticia.text, \"lxml\")\n",
    "\n",
    "        data_noticia = soup_noticia.select(\"[type='application/ld+json']\")[0]\n",
    "        try: # Si tiene descripción y cuerpo de noticia\n",
    "            noticia = [json.loads(data_noticia.text)[0]['description'], json.loads(data_noticia.text)[0]['articleBody']]\n",
    "        except:\n",
    "            try: # si solo tiene cuerpo de noticia\n",
    "                noticia = [json.loads(data_noticia.text)[0]['articleBody']]\n",
    "            except:\n",
    "                noticia = ['Revisar']\n",
    "\n",
    "        texto.append(' '.join(noticia))\n",
    "        \n",
    "    else:\n",
    "        continue\n",
    "\n",
    "\n",
    "# Hacemos un diccionario con todos los resultados obtenidos y lo pasamos a formato dataframe\n",
    "table_dict = {'Web': web,\\\n",
    "              'Título': titulo,\\\n",
    "              'Autor': por,\\\n",
    "              'Cabecera': cab,\\\n",
    "              'Comentarios': com,\\\n",
    "              'Texto': texto,\\\n",
    "              'Página' : pag,\\\n",
    "              'URL': url,\n",
    "              'Fecha' : fecha}\n",
    "lr = pd.DataFrame(table_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cinco Días"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL de la que partimos\n",
    "url = 'https://cincodias.elpais.com/'\n",
    "peticion = requests.get(url, allow_redirects = False)\n",
    "soup = BeautifulSoup(peticion.text, \"lxml\")\n",
    "\n",
    "# Inicializamos listas vacías para llenarlas con bucles\n",
    "web = [] # será siempre \"Cinco Días\"\n",
    "titulo = [] # para el título de la noticia\n",
    "por = [] # para el autor \n",
    "cab = [] # para indicar si el artículo está en la cabecera o no. Binario (1 = sí está en cabecera, 0 = no)\n",
    "com = [] # para el número de comentarios que tiene\n",
    "texto = [] # para el texto de la noticia\n",
    "pag = [] # para el número de página en el que está la noticia \n",
    "url = [] # para la url de la noticia, que sería la específica que el sistema le recomendaría (o no) al usuario\n",
    "fecha = []\n",
    "\n",
    "for i in range(0, len(soup.findAll('article'))):\n",
    "    web.append('Cinco Días')\n",
    "    \n",
    "    if i == 0:\n",
    "        cab.append(1)\n",
    "    else:\n",
    "        cab.append(0)\n",
    "    \n",
    "    pag.append(1)\n",
    "    \n",
    "    # Titular\n",
    "    try:\n",
    "        titulo.append(re.findall(r'\\n(.*)\\n', soup.findAll('article')[i].find('h2').text)[0])\n",
    "    except:\n",
    "        titulo.append(soup.findAll('article')[i].find('h2').text)\n",
    "        \n",
    "    # Autor\n",
    "    try:\n",
    "        por.append(soup.findAll('article')[i].find('span', {'class' : 'autor-nombre'}).text)\n",
    "    except:\n",
    "        por.append(None)\n",
    "        \n",
    "    # Comentarios: None. No hay en esta página\n",
    "    com.append(None) \n",
    "    \n",
    "    # URL\n",
    "    urll = soup.findAll('article')[i].find('h2').find('a').get('href')\n",
    "    \n",
    "    if urll[0:2] == '/c':\n",
    "        urll = 'https://cincodias.elpais.com' + urll\n",
    "    elif urll[0:2] == '//':\n",
    "        urll = 'https:' + urll\n",
    "    elif urll[0:2] == '/r':\n",
    "        urll = 'https://retina.elpais.com/' + urll\n",
    "    else:\n",
    "        urll = urll\n",
    "    \n",
    "    url.append(urll)\n",
    "    \n",
    "    # Texto\n",
    "    \n",
    "    if urll[8:14] != 'retina':\n",
    "        try:\n",
    "            peticion_noticia = requests.get(urll, allow_redirects = False)\n",
    "            soup_noticia = BeautifulSoup(peticion_noticia.text, \"lxml\")\n",
    "\n",
    "            try:\n",
    "                h1 = soup_noticia.findAll('article')[0].findAll('h1')[0].text\n",
    "            except:\n",
    "                h1 = ''\n",
    "\n",
    "            try:\n",
    "                h2 = soup_noticia.findAll('article')[0].findAll('h2')[0].text\n",
    "            except:\n",
    "                h2 = ''\n",
    "\n",
    "            cuerpo = re.findall(r'(.*)\\n', soup_noticia.findAll('article')[0].find('div', {'class' : 'articulo-cuerpo'}).text)\n",
    "\n",
    "            tex = [h1, h2, ' '.join(cuerpo)]\n",
    "            tex = ' '.join(tex)\n",
    "            texto.append(tex)\n",
    "        except:\n",
    "            texto.append('Revisar')\n",
    "    else:\n",
    "        try:\n",
    "            peticion_noticia = requests.get(urll, allow_redirects = False)\n",
    "            soup_noticia = BeautifulSoup(peticion_noticia.text, \"lxml\")\n",
    "\n",
    "            cuerpo = soup_noticia.findAll('div', {'class' : 'articulo-cuerpo'})[0].text.replace('\\n', '')\n",
    "            texto.append(cuerpo)\n",
    "        except:\n",
    "            texto.append('Revisar2')\n",
    "\n",
    "    \n",
    "    # Fecha\n",
    "    fecha.append(None)\n",
    "    \n",
    "\n",
    "# Hacemos un diccionario con todos los resultados obtenidos y lo pasamos a formato dataframe de Pandas\n",
    "\n",
    "table_dict = {'Web': web,\\\n",
    "              'Título': titulo,\\\n",
    "              'Autor': por,\\\n",
    "              'Cabecera': cab,\\\n",
    "              'Comentarios': com,\\\n",
    "              'Texto': texto,\\\n",
    "              'Página' : pag,\\\n",
    "              'URL': url,\\\n",
    "              'Fecha': fecha}\n",
    "\n",
    "\n",
    "cd = pd.DataFrame(table_dict) \n",
    "\n",
    "# Fecha\n",
    "r = re.compile(r'(\\d+/\\d+/\\d+)')\n",
    "\n",
    "def match(x):\n",
    "    m = r.search(x)\n",
    "    if m:\n",
    "        return  m.group()\n",
    "    return  \"\"  \n",
    "\n",
    "cd['Fecha'] = cd[\"URL\"].apply(match) # aplicamos la función anterior a la columna \"URL\" \n",
    "cd['Fecha'] = cd['Fecha'].apply(lambda s: \"{}-{}-{}\".format(s[0:4], s[5:7], s[8:]))\n",
    "\n",
    "cd = cd[cd['Texto'] != 'Revisar']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## As"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL de la que partimos\n",
    "url = 'https://as.com/'\n",
    "peticion = requests.get(url, allow_redirects = False)\n",
    "soup = BeautifulSoup(peticion.text, \"lxml\")\n",
    "\n",
    "# Inicializamos listas vacías para llenarlas con bucles\n",
    "web = [] # será siempre \"As\"\n",
    "titulo = [] # para el título de la noticia\n",
    "por = [] # para el autor \n",
    "cab = [] # para indicar si el artículo está en la cabecera o no. Binario (1 = sí está en cabecera, 0 = no)\n",
    "com = [] # para el número de comentarios que tiene\n",
    "texto = [] # para el texto de la noticia\n",
    "pag = [] # para el número de página en el que está la noticia \n",
    "url = [] # para la url de la noticia, que sería la específica que el sistema le recomendaría (o no) al usuario\n",
    "fecha = []\n",
    "\n",
    "\n",
    "for i in range(0, len(soup.findAll('article'))):\n",
    "    web.append('As')\n",
    "    \n",
    "    # Cabecera\n",
    "    if i == 0:\n",
    "        cab.append(1)\n",
    "    else:\n",
    "        cab.append(0)\n",
    "        \n",
    "    # Página siempre 1\n",
    "    pag.append(1)\n",
    "    \n",
    "    # Comentarios\n",
    "    try:\n",
    "        com.append(soup.findAll('article')[i].find('span', {'class' : 'comment-n'}).text)\n",
    "    except:\n",
    "        com.append(None)\n",
    "    \n",
    "    # URL\n",
    "    try:\n",
    "        urll = soup.findAll('article')[i].find('h2').find('a').get('href')\n",
    "    except:\n",
    "        try:\n",
    "            urll = soup.findAll('article')[i].find('h3').find('a').get('href')\n",
    "        except:\n",
    "            try:\n",
    "                urll = soup.findAll('article')[i].find('h4').find('a').get('href')\n",
    "            except:\n",
    "                urll = 'Revisar'\n",
    "    \n",
    "    url.append(urll)\n",
    "    \n",
    "    # Título\n",
    "    try:\n",
    "        titulo.append(re.findall(r'\\n(.*)\\n', soup.findAll('article')[i].find('h2').text)[0])\n",
    "    except:\n",
    "        try:\n",
    "            titulo.append(re.findall(r'\\n(.*)\\n', soup.findAll('article')[i].find('h3').text)[0])\n",
    "        except:\n",
    "            try:\n",
    "                titulo.append(re.findall(r'\\n(.*)\\n', soup.findAll('article')[i].find('h4').text)[0])\n",
    "            except:\n",
    "                titulo.append('Revisar')\n",
    "    \n",
    "    # Autor\n",
    "    try:\n",
    "        por.append(re.findall(r'\\n(.*)\\n', soup.findAll('article')[i].find('span', {'class' : \"autor-nombre\"}).text)[0])\n",
    "    except:\n",
    "        por.append(None)\n",
    "    \n",
    "    # Texto\n",
    "    try:\n",
    "        peticion_noticia = requests.get(urll, allow_redirects = False)\n",
    "        soup_noticia = BeautifulSoup(peticion_noticia.text, \"lxml\")\n",
    "    \n",
    "        try:\n",
    "            texto.append(soup_noticia.findAll('div', {'id' : 'cuerpo_noticia'})[0].text.replace('\\n', '').replace('\\t', ''))\n",
    "        except:\n",
    "            try:\n",
    "                texto.append(soup_noticia.findAll('div', {'class' : 'stream-ev-wr'})[0].text.replace('\\n', '').replace('\\t', ''))\n",
    "            except:\n",
    "                texto.append(None)\n",
    "    except:\n",
    "        texto.append(None)\n",
    "    \n",
    "    # Fecha\n",
    "    try:\n",
    "        fecha.append(soup.findAll('article')[i].find('span', {'class' : 'fecha'}).text)\n",
    "    except:\n",
    "        fecha.append(None)\n",
    "\n",
    "\n",
    "table_dict = {'Web': web,\\\n",
    "              'Título': titulo,\\\n",
    "              'Autor': por,\\\n",
    "              'Cabecera': cab,\\\n",
    "              'Comentarios': com,\\\n",
    "              'Texto': texto,\\\n",
    "              'Página' : pag,\\\n",
    "              'URL': url,\\\n",
    "              'Fecha': fecha}\n",
    "\n",
    "\n",
    "df_as = pd.DataFrame(table_dict) \n",
    "    \n",
    "df_as['Fecha'] = pd.to_datetime(df_as['Fecha']).dt.strftime('%Y-%m-%d')\n",
    "\n",
    "df_as = df_as.dropna(how='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expansión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL de la que partimos\n",
    "url = 'https://www.expansion.com/'\n",
    "peticion = requests.get(url, allow_redirects = False)\n",
    "soup = BeautifulSoup(peticion.text, \"lxml\")\n",
    "\n",
    "# Inicializamos listas vacías para llenarlas con bucles\n",
    "web = [] # será siempre \"Expansión\"\n",
    "titulo = [] # para el título de la noticia\n",
    "por = [] # para el autor \n",
    "cab = [] # para indicar si el artículo está en la cabecera o no. Binario (1 = sí está en cabecera, 0 = no)\n",
    "com = [] # para el número de comentarios que tiene\n",
    "texto = [] # para el texto de la noticia\n",
    "pag = [] # para el número de página en el que está la noticia\n",
    "url = [] # para la url de la noticia, que sería la específica que el sistema le recomendaría (o no) al usuario\n",
    "fecha = []\n",
    "\n",
    "for i in range(0, len(soup.findAll('article'))):\n",
    "    web.append('Expansión')\n",
    "    \n",
    "    # Titular\n",
    "    titulo.append(soup.findAll('article')[i].find('h2').text)\n",
    "    \n",
    "    # Autor\n",
    "    try:\n",
    "        por.append(soup.findAll('article')[i].find('span', {'class' : 'ue-c-cover-content__byline-name'}).text.replace('\\n', '').replace('Redacción: ', ''))\n",
    "    except:\n",
    "        por.append(None)\n",
    "    \n",
    "    # Cabecera\n",
    "    if i == 0:\n",
    "        cab.append(1)\n",
    "    else:\n",
    "        cab.append(0)\n",
    "        \n",
    "    # Página\n",
    "    pag.append(1)\n",
    "    \n",
    "    # Comentarios: no hay\n",
    "    com.append(None)\n",
    "    \n",
    "    # URL\n",
    "    urll = soup.findAll('article')[i].find('h2').find('a').get('href')\n",
    "    url.append(urll)\n",
    "    \n",
    "    # Texto\n",
    "    \n",
    "    try:\n",
    "        peticion_noticia = requests.get(urll, allow_redirects = False)\n",
    "        soup_noticia = BeautifulSoup(peticion_noticia.text, \"lxml\")\n",
    "\n",
    "        try:\n",
    "            try:\n",
    "                stand_first = soup_noticia.findAll('p', {'class' : 'ue-c-article__standfirst'})[0].text\n",
    "            except:\n",
    "                stand_first = None\n",
    "            \n",
    "            tex = []\n",
    "            for i in range(0, len(soup_noticia.findAll('div', {'class' : 'ue-c-article__body'})[0].findAll('p', {'class' : ''}))):\n",
    "                tex.append(soup_noticia.findAll('div', {'class' : 'ue-c-article__body'})[0].findAll('p', {'class' : ''})[i].text)\n",
    "\n",
    "            tex = ' '.join(tex)\n",
    "            tex_prev = [stand_first, tex]\n",
    "\n",
    "            texto.append(' '.join(tex_prev))\n",
    "        except:\n",
    "            texto.append(None)\n",
    "    except:\n",
    "        texto.append(None)\n",
    "    \n",
    "    # Fecha\n",
    "    fecha.append('Revisar')\n",
    "\n",
    "\n",
    "# Hacemos un diccionario con todos los resultados obtenidos y lo pasamos a formato dataframe de Pandas\n",
    "\n",
    "table_dict = {'Web': web,\\\n",
    "              'Título': titulo,\\\n",
    "              'Autor': por,\\\n",
    "              'Cabecera': cab,\\\n",
    "              'Comentarios': com,\\\n",
    "              'Texto': texto,\\\n",
    "              'Página' : pag,\\\n",
    "              'URL': url}\n",
    "\n",
    "ex = pd.DataFrame(table_dict) \n",
    "\n",
    "# Fecha\n",
    "r = re.compile(r'(\\d+/\\d+/\\d+)')\n",
    "\n",
    "def match(x):\n",
    "    m = r.search(x)\n",
    "    if m:\n",
    "        return  m.group()\n",
    "    return  \"\"  \n",
    "\n",
    "ex['Fecha'] = ex[\"URL\"].apply(match) # aplicamos la función anterior a la columna \"URL\" \n",
    "ex['Fecha'] = ex['Fecha'].apply(lambda s: \"{}-{}-{}\".format(s[0:4], s[5:7], s[8:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataframe conjunto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataframe uniendo todas las noticias anteriores.  \n",
    "Saca algunas líneas \"feas\" adicionales, hay que hacer limpieza antes de intentar crear algún algoritmo, igual que al final del texto de la noticia también añade cosas adicionales (ver enlaces de interés, suscríbete, gracias por leer, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>new_id</th>\n",
       "      <th>Web</th>\n",
       "      <th>Título</th>\n",
       "      <th>Autor</th>\n",
       "      <th>Cabecera</th>\n",
       "      <th>Comentarios</th>\n",
       "      <th>Texto</th>\n",
       "      <th>Página</th>\n",
       "      <th>URL</th>\n",
       "      <th>Fecha</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gen_1</td>\n",
       "      <td>El Confidencial</td>\n",
       "      <td>Los alcaldes aplazan la decisión de la hucha d...</td>\n",
       "      <td>EFE</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>31/07/2020 12:06 -                            ...</td>\n",
       "      <td>1</td>\n",
       "      <td>https://www.elconfidencial.com/espana/2020-07-...</td>\n",
       "      <td>2020-07-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gen_2</td>\n",
       "      <td>El Confidencial</td>\n",
       "      <td>Sánchez concede a Urkullu 2.000 millones más p...</td>\n",
       "      <td>A. Pérez Giménez</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>31/07/2020 11:00 -                            ...</td>\n",
       "      <td>1</td>\n",
       "      <td>https://www.elconfidencial.com/espana/2020-07-...</td>\n",
       "      <td>2020-07-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gen_3</td>\n",
       "      <td>El Confidencial</td>\n",
       "      <td>Interior compra 1.300 'mascarillas chic' solo ...</td>\n",
       "      <td>Roberto R. Ballesteros</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>31/07/2020 05:00Adelantado en La Dirección Gen...</td>\n",
       "      <td>1</td>\n",
       "      <td>https://www.elconfidencial.com/espana/2020-07-...</td>\n",
       "      <td>2020-07-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gen_4</td>\n",
       "      <td>El Confidencial</td>\n",
       "      <td>Sanidad dice que es público el comité que Ribe...</td>\n",
       "      <td>Darío Ojeda</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>31/07/2020 05:00Adelantado en Tres meses despu...</td>\n",
       "      <td>1</td>\n",
       "      <td>https://www.elconfidencial.com/espana/2020-07-...</td>\n",
       "      <td>2020-07-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gen_5</td>\n",
       "      <td>El Confidencial</td>\n",
       "      <td>El 50,5% de los catalanes rechaza la independe...</td>\n",
       "      <td>Europa Press</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>31/07/2020 12:20 -                            ...</td>\n",
       "      <td>1</td>\n",
       "      <td>https://www.elconfidencial.com/espana/cataluna...</td>\n",
       "      <td>2020-07-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>864</th>\n",
       "      <td>eco_865</td>\n",
       "      <td>Expansión</td>\n",
       "      <td>El déficit del Estado se quintuplica hasta 48....</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>El déficit del Estado alcanzó en el primer sem...</td>\n",
       "      <td>1</td>\n",
       "      <td>https://www.expansion.com/economia/2020/07/30/...</td>\n",
       "      <td>2020-07-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>865</th>\n",
       "      <td>eco_866</td>\n",
       "      <td>Expansión</td>\n",
       "      <td>El rover Perseverance de la NASA parte rumbo a...</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>https://videos.expansion.com/v/0_udydlkib-el-r...</td>\n",
       "      <td>--</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>866</th>\n",
       "      <td>eco_867</td>\n",
       "      <td>Expansión</td>\n",
       "      <td>Videoanálisis técnico: Precaución en los 7.040...</td>\n",
       "      <td>ROBERTO MO...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>https://videos.expansion.com/v/0_wyr4zvsq-vide...</td>\n",
       "      <td>--</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>867</th>\n",
       "      <td>eco_868</td>\n",
       "      <td>Expansión</td>\n",
       "      <td>Esto es lo que pasa si no lavas o cambias la m...</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>https://videos.expansion.com/v/0_kiw8z5bi-esto...</td>\n",
       "      <td>--</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>868</th>\n",
       "      <td>eco_869</td>\n",
       "      <td>Expansión</td>\n",
       "      <td>Guía TV, programación para hoy en televisión</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>https://sincroguia-tv.expansion.com/</td>\n",
       "      <td>--</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>869 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      new_id              Web  \\\n",
       "0      gen_1  El Confidencial   \n",
       "1      gen_2  El Confidencial   \n",
       "2      gen_3  El Confidencial   \n",
       "3      gen_4  El Confidencial   \n",
       "4      gen_5  El Confidencial   \n",
       "..       ...              ...   \n",
       "864  eco_865        Expansión   \n",
       "865  eco_866        Expansión   \n",
       "866  eco_867        Expansión   \n",
       "867  eco_868        Expansión   \n",
       "868  eco_869        Expansión   \n",
       "\n",
       "                                                Título  \\\n",
       "0    Los alcaldes aplazan la decisión de la hucha d...   \n",
       "1    Sánchez concede a Urkullu 2.000 millones más p...   \n",
       "2    Interior compra 1.300 'mascarillas chic' solo ...   \n",
       "3    Sanidad dice que es público el comité que Ribe...   \n",
       "4    El 50,5% de los catalanes rechaza la independe...   \n",
       "..                                                 ...   \n",
       "864  El déficit del Estado se quintuplica hasta 48....   \n",
       "865  El rover Perseverance de la NASA parte rumbo a...   \n",
       "866  Videoanálisis técnico: Precaución en los 7.040...   \n",
       "867  Esto es lo que pasa si no lavas o cambias la m...   \n",
       "868       Guía TV, programación para hoy en televisión   \n",
       "\n",
       "                                                 Autor  Cabecera Comentarios  \\\n",
       "0                                                  EFE         1           0   \n",
       "1                                     A. Pérez Giménez         0          22   \n",
       "2                               Roberto R. Ballesteros         0           5   \n",
       "3                                          Darío Ojeda         0           5   \n",
       "4                                         Europa Press         0           0   \n",
       "..                                                 ...       ...         ...   \n",
       "864                                               None         0        None   \n",
       "865                                               None         0        None   \n",
       "866                                      ROBERTO MO...         0        None   \n",
       "867                                               None         0        None   \n",
       "868                                               None         0        None   \n",
       "\n",
       "                                                 Texto  Página  \\\n",
       "0    31/07/2020 12:06 -                            ...       1   \n",
       "1    31/07/2020 11:00 -                            ...       1   \n",
       "2    31/07/2020 05:00Adelantado en La Dirección Gen...       1   \n",
       "3    31/07/2020 05:00Adelantado en Tres meses despu...       1   \n",
       "4    31/07/2020 12:20 -                            ...       1   \n",
       "..                                                 ...     ...   \n",
       "864  El déficit del Estado alcanzó en el primer sem...       1   \n",
       "865                                               None       1   \n",
       "866                                               None       1   \n",
       "867                                               None       1   \n",
       "868                                               None       1   \n",
       "\n",
       "                                                   URL       Fecha  \n",
       "0    https://www.elconfidencial.com/espana/2020-07-...  2020-07-31  \n",
       "1    https://www.elconfidencial.com/espana/2020-07-...  2020-07-31  \n",
       "2    https://www.elconfidencial.com/espana/2020-07-...  2020-07-31  \n",
       "3    https://www.elconfidencial.com/espana/2020-07-...  2020-07-31  \n",
       "4    https://www.elconfidencial.com/espana/cataluna...  2020-07-31  \n",
       "..                                                 ...         ...  \n",
       "864  https://www.expansion.com/economia/2020/07/30/...  2020-07-30  \n",
       "865  https://videos.expansion.com/v/0_udydlkib-el-r...          --  \n",
       "866  https://videos.expansion.com/v/0_wyr4zvsq-vide...          --  \n",
       "867  https://videos.expansion.com/v/0_kiw8z5bi-esto...          --  \n",
       "868               https://sincroguia-tv.expansion.com/          --  \n",
       "\n",
       "[869 rows x 10 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lista con los dataframes que tenemos de los diferentes periódicos\n",
    "news = [ec, em, ep, abc, ee, m, lr, cd, df_as, ex]\n",
    "\n",
    "# Los unimos, y para que no se repitan los índices, los reseteamos\n",
    "df = pd.concat(news).reset_index(drop=True)\n",
    "\n",
    "# Para crear el id (único) de cada noticia\n",
    "\n",
    "generales = ['El Confidencial', 'El Mundo', 'El País', 'ABC', 'La Razón']\n",
    "economicos = ['El Economista', 'Cinco Días', 'Expansión']\n",
    "deportivos = ['Marca', 'As']\n",
    "\n",
    "def tipo_diario(row):\n",
    "    if row['Web'] in generales:\n",
    "        val = 'gen_' \n",
    "    elif row['Web'] in economicos:\n",
    "        val = 'eco_'\n",
    "    elif row['Web'] in deportivos:\n",
    "        val = 'dep_' \n",
    "    else:\n",
    "        val = -1\n",
    "    return val\n",
    "\n",
    "df['new_id'] = df.apply(tipo_diario, axis=1)\n",
    "indice = (df.index + 1).astype(str)\n",
    "df['new_id'] = df['new_id'] + indice\n",
    "\n",
    "# Poner la columna de ID en la primera posición\n",
    "new_id = df['new_id']\n",
    "df.drop(labels=['new_id'], axis=1, inplace = True)\n",
    "df.insert(0, 'new_id', new_id)\n",
    "\n",
    "# Ver el dataframe final\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "869"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exportar a csv\n",
    "nombre = 'news_completo_' + datetime.today().strftime('%d%m%Y') + '.csv'\n",
    "df.to_csv(nombre, encoding='utf-8-sig', header = True, sep = ';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clientes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5 modalidades diferentes de suscripción:  \n",
    "\n",
    "- Alpha: suscripción básica (diarios generalistas)\n",
    "- Beta: diarios generalistas + especializados en deportes\n",
    "- Gamma: diarios generalistas + especializados en economía/finanzas\n",
    "- Delta: suscripción \"familiar\", incluye la Omega x 4 usuarios\n",
    "- Omega: suscripción completa (diarios generalistas + deportes + economía/finanzas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataframe de LECTURAS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tras obtener el dataframe completo con todas las noticias (df), creamos otro dataframe con 45.000 clientes, indicando cuáles de esas noticias han leído (1) y cuáles no (0). \n",
    "Para que todos los usuarios no sean iguales, creamos 9 perfiles diferentes, con distinta probabilidad de que lean una noticia o no (de forma aleatoria), para tener clientes que leen mucho y otros que leen poco.\n",
    "Generamos cada \"sub-dataframe\" en función de cada una de las diferentes suscripciones que se ofrecen, ya que por ejemplo las noticias de diarios deportivos no estarán disponibles para la suscripción Alpha (diarios generales)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "# Las columnas del dataframe serán todas las noticias que provienen del df de noticias\n",
    "columnas = df['new_id']\n",
    "\n",
    "# Asumimos reparto equitativo de los clientes entre las 5 suscripciones\n",
    "prob = [0.2, 0.2, 0.2, 0.2, 0.2]\n",
    "suscripciones = ['alpha', 'beta', 'gamma', 'delta', 'omega']\n",
    "\n",
    "# Creamos 45.000 clientes, en principio con números aleatorios para todas las noticias\n",
    "clientes_0 = pd.DataFrame(np.random.choice(np.arange(0, 2), p=[0.1, 0.9], size=(45000, len(columnas))), columns = columnas)\n",
    "\n",
    "# 9.000 en cada segmento\n",
    "clientes_alpha, clientes_beta, clientes_gamma, clientes_delta, clientes_omega = np.split(clientes_0, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ALPHA: suscripción a diarios generalistas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>new_id</th>\n",
       "      <th>suscripcion</th>\n",
       "      <th>gen_1</th>\n",
       "      <th>gen_2</th>\n",
       "      <th>gen_3</th>\n",
       "      <th>gen_4</th>\n",
       "      <th>gen_5</th>\n",
       "      <th>gen_6</th>\n",
       "      <th>gen_7</th>\n",
       "      <th>gen_8</th>\n",
       "      <th>gen_9</th>\n",
       "      <th>...</th>\n",
       "      <th>gen_574</th>\n",
       "      <th>gen_575</th>\n",
       "      <th>gen_576</th>\n",
       "      <th>gen_577</th>\n",
       "      <th>gen_578</th>\n",
       "      <th>gen_579</th>\n",
       "      <th>gen_580</th>\n",
       "      <th>gen_581</th>\n",
       "      <th>gen_582</th>\n",
       "      <th>gen_583</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>alpha</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>alpha</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>alpha</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>alpha</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>alpha</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8995</th>\n",
       "      <td>alpha</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8996</th>\n",
       "      <td>alpha</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8997</th>\n",
       "      <td>alpha</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8998</th>\n",
       "      <td>alpha</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8999</th>\n",
       "      <td>alpha</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9000 rows × 450 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "new_id suscripcion  gen_1  gen_2  gen_3  gen_4  gen_5  gen_6  gen_7  gen_8  \\\n",
       "0            alpha      1      1      1      0      0      1      1      1   \n",
       "1            alpha      1      1      1      1      1      0      1      1   \n",
       "2            alpha      1      1      1      1      1      1      1      1   \n",
       "3            alpha      1      1      1      0      1      1      1      0   \n",
       "4            alpha      1      1      1      1      1      1      1      1   \n",
       "...            ...    ...    ...    ...    ...    ...    ...    ...    ...   \n",
       "8995         alpha      0      0      0      0      0      0      0      0   \n",
       "8996         alpha      0      0      0      0      0      0      1      0   \n",
       "8997         alpha      0      0      0      0      0      0      1      0   \n",
       "8998         alpha      0      1      0      0      0      0      0      0   \n",
       "8999         alpha      0      0      0      0      0      0      0      0   \n",
       "\n",
       "new_id  gen_9  ...  gen_574  gen_575  gen_576  gen_577  gen_578  gen_579  \\\n",
       "0           0  ...        1        1        1        1        1        1   \n",
       "1           1  ...        1        0        1        1        1        1   \n",
       "2           1  ...        1        1        1        1        1        1   \n",
       "3           1  ...        1        1        1        1        0        1   \n",
       "4           1  ...        1        1        1        1        1        1   \n",
       "...       ...  ...      ...      ...      ...      ...      ...      ...   \n",
       "8995        0  ...        0        0        1        0        0        0   \n",
       "8996        0  ...        0        0        0        0        0        0   \n",
       "8997        0  ...        0        0        0        0        0        0   \n",
       "8998        0  ...        1        0        0        0        1        1   \n",
       "8999        0  ...        0        0        0        0        0        0   \n",
       "\n",
       "new_id  gen_580  gen_581  gen_582  gen_583  \n",
       "0             1        1        1        1  \n",
       "1             1        1        1        1  \n",
       "2             1        1        1        1  \n",
       "3             1        1        1        1  \n",
       "4             1        1        0        1  \n",
       "...         ...      ...      ...      ...  \n",
       "8995          0        0        0        0  \n",
       "8996          0        0        0        0  \n",
       "8997          0        0        0        0  \n",
       "8998          0        0        0        0  \n",
       "8999          0        0        0        0  \n",
       "\n",
       "[9000 rows x 450 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Columnas del dataframe de noticias que aplican a este grupo; solo las generales\n",
    "cols_alpha = [col for col in clientes_0 if col.startswith('gen')]\n",
    "\n",
    "cols = clientes_0.columns.values\n",
    "\n",
    "# Máscara para identificar las noticias que están disponibles en esta suscripción\n",
    "mask_alpha = []\n",
    "\n",
    "for i in range(0, len(cols)):\n",
    "    mask_alpha.append(cols[i] in cols_alpha)\n",
    "\n",
    "# Máscara para identificar las noticias que NO están disponibles en esta suscripción\n",
    "mask_no_alpha = [not i for i in mask_alpha]\n",
    "\n",
    "# Las noticias que no están disponibles se quedan en nulo, para diferenciarlas de las que sí están disponibles pero no se han \n",
    "# leído (0)\n",
    "clientes_alpha[clientes_alpha.columns[mask_no_alpha]] = None\n",
    "\n",
    "# Creamos dataframe de clientes_alpha\n",
    "ca = clientes_alpha[clientes_alpha.columns[mask_alpha]].copy()\n",
    "\n",
    "# Diferentes perfiles\n",
    "ca.loc[0:999,] = np.random.choice(np.arange(0, 2), p=[0.1, 0.9], size=(1000, len(clientes_alpha.columns[mask_alpha]))) \n",
    "ca.loc[1000:1999,] = np.random.choice(np.arange(0, 2), p=[0.2, 0.8], size=(1000, len(clientes_alpha.columns[mask_alpha]))) \n",
    "ca.loc[2000:2999,] = np.random.choice(np.arange(0, 2), p=[0.3, 0.7], size=(1000, len(clientes_alpha.columns[mask_alpha]))) \n",
    "ca.loc[3000:3999,] = np.random.choice(np.arange(0, 2), p=[0.4, 0.6], size=(1000, len(clientes_alpha.columns[mask_alpha]))) \n",
    "ca.loc[4000:4999,] = np.random.choice(np.arange(0, 2), p=[0.5, 0.5], size=(1000, len(clientes_alpha.columns[mask_alpha]))) \n",
    "ca.loc[5000:5999,] = np.random.choice(np.arange(0, 2), p=[0.6, 0.4], size=(1000, len(clientes_alpha.columns[mask_alpha]))) \n",
    "ca.loc[6000:6999,] = np.random.choice(np.arange(0, 2), p=[0.7, 0.3], size=(1000, len(clientes_alpha.columns[mask_alpha]))) \n",
    "ca.loc[7000:7999,] = np.random.choice(np.arange(0, 2), p=[0.8, 0.2], size=(1000, len(clientes_alpha.columns[mask_alpha]))) \n",
    "ca.loc[8000:8999,] = np.random.choice(np.arange(0, 2), p=[0.9, 0.1], size=(1000, len(clientes_alpha.columns[mask_alpha]))) \n",
    "\n",
    "ca.insert(0, 'suscripcion', 'alpha')\n",
    "ca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       393\n",
       "1       397\n",
       "2       402\n",
       "3       405\n",
       "4       402\n",
       "       ... \n",
       "8995     47\n",
       "8996     41\n",
       "8997     27\n",
       "8998     51\n",
       "8999     43\n",
       "Length: 9000, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Comprobación de que leen diferente candiad de noticias\n",
    "ca.sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beta: diarios generalistas + especializados en deportes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columnas del dataframe de noticias que aplican a este grupo; las generales y las deportivas\n",
    "cols_beta = [col for col in clientes_0 if (col.startswith('gen') or col.startswith('dep'))]\n",
    "\n",
    "# Máscara para identificar las noticias que están disponibles en esta suscripción\n",
    "mask_beta = []\n",
    "\n",
    "for i in range(0, len(cols)):\n",
    "    mask_beta.append(cols[i] in cols_beta)\n",
    "\n",
    "# Máscara para identificar las noticias que NO están disponibles en esta suscripción\n",
    "mask_no_beta = [not i for i in mask_beta]\n",
    "\n",
    "# Las noticias que no están disponibles se quedan en nulo, para diferenciarlas de las que sí están disponibles pero no se han \n",
    "# leído (0)\n",
    "clientes_beta[clientes_beta.columns[mask_no_beta]] = None\n",
    "\n",
    "# Creamos dataframe de clientes_beta\n",
    "cb = clientes_beta[clientes_beta.columns[mask_beta]].copy()\n",
    "\n",
    "# Diferentes perfiles\n",
    "cb.loc[9000:9999,] = np.random.choice(np.arange(0, 2), p=[0.1, 0.9], size=(1000, len(clientes_beta.columns[mask_beta]))) \n",
    "cb.loc[10000:10999,] = np.random.choice(np.arange(0, 2), p=[0.2, 0.8], size=(1000, len(clientes_beta.columns[mask_beta]))) \n",
    "cb.loc[11000:11999,] = np.random.choice(np.arange(0, 2), p=[0.3, 0.7], size=(1000, len(clientes_beta.columns[mask_beta]))) \n",
    "cb.loc[12000:12999,] = np.random.choice(np.arange(0, 2), p=[0.4, 0.6], size=(1000, len(clientes_beta.columns[mask_beta]))) \n",
    "cb.loc[13000:13999,] = np.random.choice(np.arange(0, 2), p=[0.5, 0.5], size=(1000, len(clientes_beta.columns[mask_beta]))) \n",
    "cb.loc[14000:14999,] = np.random.choice(np.arange(0, 2), p=[0.6, 0.4], size=(1000, len(clientes_beta.columns[mask_beta]))) \n",
    "cb.loc[15000:15999,] = np.random.choice(np.arange(0, 2), p=[0.7, 0.3], size=(1000, len(clientes_beta.columns[mask_beta]))) \n",
    "cb.loc[16000:16999,] = np.random.choice(np.arange(0, 2), p=[0.8, 0.2], size=(1000, len(clientes_beta.columns[mask_beta]))) \n",
    "cb.loc[17000:17999,] = np.random.choice(np.arange(0, 2), p=[0.9, 0.1], size=(1000, len(clientes_beta.columns[mask_beta]))) \n",
    "\n",
    "cb.insert(0, 'suscripcion', 'beta')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gamma: diarios generalistas + especializados en economía/finanzas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columnas del dataframe de noticias que aplican a este grupo; las generales y las de economía \n",
    "cols_gamma = [col for col in clientes_0 if (col.startswith('gen') or col.startswith('eco'))]\n",
    "\n",
    "# Máscara para identificar las noticias que están disponibles en esta suscripción\n",
    "mask_gamma = []\n",
    "\n",
    "for i in range(0, len(cols)):\n",
    "    mask_gamma.append(cols[i] in cols_gamma)\n",
    "\n",
    "# Máscara para identificar las noticias que NO están disponibles en esta suscripción\n",
    "mask_no_gamma = [not i for i in mask_gamma]\n",
    "\n",
    "# Las noticias que no están disponibles se quedan en nulo, para diferenciarlas de las que sí están disponibles pero no se han \n",
    "# leído (0)\n",
    "clientes_gamma[clientes_gamma.columns[mask_no_gamma]] = None\n",
    "\n",
    "# Creamos dataframe de clientes_gamma\n",
    "cg = clientes_gamma[clientes_gamma.columns[mask_gamma]].copy()\n",
    "\n",
    "# Diferentes perfiles\n",
    "cg.loc[18000:18999,] = np.random.choice(np.arange(0, 2), p=[0.1, 0.9], size=(1000, len(clientes_gamma.columns[mask_gamma]))) \n",
    "cg.loc[19000:19999,] = np.random.choice(np.arange(0, 2), p=[0.2, 0.8], size=(1000, len(clientes_gamma.columns[mask_gamma]))) \n",
    "cg.loc[20000:20999,] = np.random.choice(np.arange(0, 2), p=[0.3, 0.7], size=(1000, len(clientes_gamma.columns[mask_gamma]))) \n",
    "cg.loc[21000:21999,] = np.random.choice(np.arange(0, 2), p=[0.4, 0.6], size=(1000, len(clientes_gamma.columns[mask_gamma]))) \n",
    "cg.loc[22000:22999,] = np.random.choice(np.arange(0, 2), p=[0.5, 0.5], size=(1000, len(clientes_gamma.columns[mask_gamma]))) \n",
    "cg.loc[23000:23999,] = np.random.choice(np.arange(0, 2), p=[0.6, 0.4], size=(1000, len(clientes_gamma.columns[mask_gamma]))) \n",
    "cg.loc[24000:24999,] = np.random.choice(np.arange(0, 2), p=[0.7, 0.3], size=(1000, len(clientes_gamma.columns[mask_gamma]))) \n",
    "cg.loc[25000:25999,] = np.random.choice(np.arange(0, 2), p=[0.8, 0.2], size=(1000, len(clientes_gamma.columns[mask_gamma]))) \n",
    "cg.loc[26000:26999,] = np.random.choice(np.arange(0, 2), p=[0.9, 0.1], size=(1000, len(clientes_gamma.columns[mask_gamma]))) \n",
    "\n",
    "cg.insert(0, 'suscripcion', 'gamma')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delta: suscripción \"familiar\", incluye la Omega (completa) x 4 usuarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columnas del dataframe de noticias que aplican a este grupo; todas, pero lo identificamos así por si en un futuro \n",
    "# decidimos modificarlo\n",
    "cols_delta = [col for col in clientes_0 if (col.startswith('gen') or col.startswith('eco') or col.startswith('dep'))]\n",
    "\n",
    "# Máscara para identificar las noticias que están disponibles en esta suscripción\n",
    "mask_delta = []\n",
    "\n",
    "for i in range(0, len(cols)):\n",
    "    mask_delta.append(cols[i] in cols_delta)\n",
    "\n",
    "# Máscara para identificar las noticias que NO están disponibles en esta suscripción\n",
    "mask_no_delta = [not i for i in mask_delta]\n",
    "\n",
    "# Las noticias que no están disponibles se quedan en nulo, para diferenciarlas de las que sí están disponibles pero no se han \n",
    "# leído (0)\n",
    "clientes_delta[clientes_delta.columns[mask_no_delta]] = None\n",
    "\n",
    "# Creamos dataframe de clientes_delta\n",
    "cd = clientes_delta[clientes_delta.columns[mask_delta]].copy()\n",
    "\n",
    "# Diferentes perfiles\n",
    "cd.loc[27000:27999,] = np.random.choice(np.arange(0, 2), p=[0.1, 0.9], size=(1000, len(clientes_delta.columns[mask_delta]))) \n",
    "cd.loc[28000:28999,] = np.random.choice(np.arange(0, 2), p=[0.2, 0.8], size=(1000, len(clientes_delta.columns[mask_delta]))) \n",
    "cd.loc[29000:29999,] = np.random.choice(np.arange(0, 2), p=[0.3, 0.7], size=(1000, len(clientes_delta.columns[mask_delta]))) \n",
    "cd.loc[30000:30999,] = np.random.choice(np.arange(0, 2), p=[0.4, 0.6], size=(1000, len(clientes_delta.columns[mask_delta]))) \n",
    "cd.loc[31000:31999,] = np.random.choice(np.arange(0, 2), p=[0.5, 0.5], size=(1000, len(clientes_delta.columns[mask_delta]))) \n",
    "cd.loc[32000:32999,] = np.random.choice(np.arange(0, 2), p=[0.6, 0.4], size=(1000, len(clientes_delta.columns[mask_delta]))) \n",
    "cd.loc[33000:33999,] = np.random.choice(np.arange(0, 2), p=[0.7, 0.3], size=(1000, len(clientes_delta.columns[mask_delta]))) \n",
    "cd.loc[34000:34999,] = np.random.choice(np.arange(0, 2), p=[0.8, 0.2], size=(1000, len(clientes_delta.columns[mask_delta]))) \n",
    "cd.loc[35000:35999,] = np.random.choice(np.arange(0, 2), p=[0.9, 0.1], size=(1000, len(clientes_delta.columns[mask_delta]))) \n",
    "\n",
    "cd.insert(0, 'suscripcion', 'delta')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Omega: suscripción completa (diarios generalistas + deportes + economía/finanzas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columnas del dataframe de noticias que aplican a este grupo; todas, pero lo identificamos así por si en un futuro \n",
    "# decidimos modificarlo\n",
    "cols_omega = [col for col in clientes_0 if (col.startswith('gen') or col.startswith('eco') or col.startswith('dep'))]\n",
    "\n",
    "# Máscara para identificar las noticias que están disponibles en esta suscripción\n",
    "mask_omega = []\n",
    "\n",
    "for i in range(0, len(cols)):\n",
    "    mask_omega.append(cols[i] in cols_omega)\n",
    "\n",
    "# Máscara para identificar las noticias que NO están disponibles en esta suscripción\n",
    "mask_no_omega = [not i for i in mask_omega]\n",
    "\n",
    "# Las noticias que no están disponibles se quedan en nulo, para diferenciarlas de las que sí están disponibles pero no se han \n",
    "# leído (0)\n",
    "clientes_omega[clientes_omega.columns[mask_no_omega]] = None\n",
    "\n",
    "# Creamos dataframe de clientes_omega\n",
    "co = clientes_omega[clientes_omega.columns[mask_omega]].copy()\n",
    "\n",
    "# Diferentes perfiles\n",
    "co.loc[36000:36999,] = np.random.choice(np.arange(0, 2), p=[0.1, 0.9], size=(1000, len(clientes_omega.columns[mask_omega]))) \n",
    "co.loc[37000:37999,] = np.random.choice(np.arange(0, 2), p=[0.2, 0.8], size=(1000, len(clientes_omega.columns[mask_omega]))) \n",
    "co.loc[38000:38999,] = np.random.choice(np.arange(0, 2), p=[0.3, 0.7], size=(1000, len(clientes_omega.columns[mask_omega]))) \n",
    "co.loc[39000:39999,] = np.random.choice(np.arange(0, 2), p=[0.4, 0.6], size=(1000, len(clientes_omega.columns[mask_omega]))) \n",
    "co.loc[40000:40999,] = np.random.choice(np.arange(0, 2), p=[0.5, 0.5], size=(1000, len(clientes_omega.columns[mask_omega]))) \n",
    "co.loc[41000:41999,] = np.random.choice(np.arange(0, 2), p=[0.6, 0.4], size=(1000, len(clientes_omega.columns[mask_omega]))) \n",
    "co.loc[42000:42999,] = np.random.choice(np.arange(0, 2), p=[0.7, 0.3], size=(1000, len(clientes_omega.columns[mask_omega]))) \n",
    "co.loc[43000:43999,] = np.random.choice(np.arange(0, 2), p=[0.8, 0.2], size=(1000, len(clientes_omega.columns[mask_omega]))) \n",
    "co.loc[44000:44999,] = np.random.choice(np.arange(0, 2), p=[0.9, 0.1], size=(1000, len(clientes_omega.columns[mask_omega]))) \n",
    "\n",
    "co.insert(0, 'suscripcion', 'omega')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataframe conjunto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>suscripcion</th>\n",
       "      <th>gen_1</th>\n",
       "      <th>gen_2</th>\n",
       "      <th>gen_3</th>\n",
       "      <th>gen_4</th>\n",
       "      <th>gen_5</th>\n",
       "      <th>gen_6</th>\n",
       "      <th>gen_7</th>\n",
       "      <th>gen_8</th>\n",
       "      <th>gen_9</th>\n",
       "      <th>...</th>\n",
       "      <th>eco_860</th>\n",
       "      <th>eco_861</th>\n",
       "      <th>eco_862</th>\n",
       "      <th>eco_863</th>\n",
       "      <th>eco_864</th>\n",
       "      <th>eco_865</th>\n",
       "      <th>eco_866</th>\n",
       "      <th>eco_867</th>\n",
       "      <th>eco_868</th>\n",
       "      <th>eco_869</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>alpha</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>alpha</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>alpha</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>alpha</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>alpha</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44995</th>\n",
       "      <td>omega</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44996</th>\n",
       "      <td>omega</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44997</th>\n",
       "      <td>omega</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44998</th>\n",
       "      <td>omega</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44999</th>\n",
       "      <td>omega</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>45000 rows × 870 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      suscripcion  gen_1  gen_2  gen_3  gen_4  gen_5  gen_6  gen_7  gen_8  \\\n",
       "0           alpha      1      1      1      0      0      1      1      1   \n",
       "1           alpha      1      1      1      1      1      0      1      1   \n",
       "2           alpha      1      1      1      1      1      1      1      1   \n",
       "3           alpha      1      1      1      0      1      1      1      0   \n",
       "4           alpha      1      1      1      1      1      1      1      1   \n",
       "...           ...    ...    ...    ...    ...    ...    ...    ...    ...   \n",
       "44995       omega      0      0      0      0      0      0      0      0   \n",
       "44996       omega      1      1      0      0      0      0      0      0   \n",
       "44997       omega      0      0      0      0      0      0      0      0   \n",
       "44998       omega      0      1      0      0      0      0      0      0   \n",
       "44999       omega      0      0      0      0      0      1      0      0   \n",
       "\n",
       "       gen_9  ...  eco_860  eco_861  eco_862  eco_863  eco_864  eco_865  \\\n",
       "0          0  ...      NaN      NaN      NaN      NaN      NaN      NaN   \n",
       "1          1  ...      NaN      NaN      NaN      NaN      NaN      NaN   \n",
       "2          1  ...      NaN      NaN      NaN      NaN      NaN      NaN   \n",
       "3          1  ...      NaN      NaN      NaN      NaN      NaN      NaN   \n",
       "4          1  ...      NaN      NaN      NaN      NaN      NaN      NaN   \n",
       "...      ...  ...      ...      ...      ...      ...      ...      ...   \n",
       "44995      0  ...      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "44996      0  ...      0.0      0.0      0.0      0.0      0.0      1.0   \n",
       "44997      0  ...      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "44998      0  ...      1.0      0.0      0.0      1.0      0.0      0.0   \n",
       "44999      0  ...      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "\n",
       "       eco_866  eco_867  eco_868  eco_869  \n",
       "0          NaN      NaN      NaN      NaN  \n",
       "1          NaN      NaN      NaN      NaN  \n",
       "2          NaN      NaN      NaN      NaN  \n",
       "3          NaN      NaN      NaN      NaN  \n",
       "4          NaN      NaN      NaN      NaN  \n",
       "...        ...      ...      ...      ...  \n",
       "44995      0.0      0.0      1.0      0.0  \n",
       "44996      0.0      0.0      0.0      0.0  \n",
       "44997      0.0      0.0      0.0      0.0  \n",
       "44998      1.0      0.0      0.0      0.0  \n",
       "44999      0.0      0.0      0.0      0.0  \n",
       "\n",
       "[45000 rows x 870 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clientes = [ca, cb, cg, cd, co]\n",
    "clientes_lectura = pd.concat(clientes).reset_index(drop=True)\n",
    "clientes_lectura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exportar a csv\n",
    "nombre = 'clientes_susc_lectura_' + datetime.today().strftime('%d%m%Y') + '.csv'\n",
    "clientes_lectura.to_csv(nombre, encoding='utf-8-sig', header = True, sep = ';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataframe de PUNTUACIONES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Asumimos que no todos los clientes que leen una noticia la puntúan (probabilidad de puntuación de noticia = 80%). Por tanto, sacamos otro dataframe con las puntuaciones de las noticias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>suscripcion</th>\n",
       "      <th>gen_1</th>\n",
       "      <th>gen_2</th>\n",
       "      <th>gen_3</th>\n",
       "      <th>gen_4</th>\n",
       "      <th>gen_5</th>\n",
       "      <th>gen_6</th>\n",
       "      <th>gen_7</th>\n",
       "      <th>gen_8</th>\n",
       "      <th>gen_9</th>\n",
       "      <th>...</th>\n",
       "      <th>eco_860</th>\n",
       "      <th>eco_861</th>\n",
       "      <th>eco_862</th>\n",
       "      <th>eco_863</th>\n",
       "      <th>eco_864</th>\n",
       "      <th>eco_865</th>\n",
       "      <th>eco_866</th>\n",
       "      <th>eco_867</th>\n",
       "      <th>eco_868</th>\n",
       "      <th>eco_869</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>alpha</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>alpha</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>None</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>alpha</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>5</td>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>alpha</td>\n",
       "      <td>None</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>None</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>alpha</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>None</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44995</th>\n",
       "      <td>omega</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44996</th>\n",
       "      <td>omega</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>4</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44997</th>\n",
       "      <td>omega</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44998</th>\n",
       "      <td>omega</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>4</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44999</th>\n",
       "      <td>omega</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>45000 rows × 870 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      suscripcion gen_1 gen_2 gen_3 gen_4 gen_5 gen_6 gen_7 gen_8 gen_9  ...  \\\n",
       "0           alpha  None     1     2  None  None     2     2     5  None  ...   \n",
       "1           alpha     4     2  None     3     2  None  None     4     2  ...   \n",
       "2           alpha     3     5  None  None  None     5  None     2     4  ...   \n",
       "3           alpha  None     4     4  None     5     2     3  None     5  ...   \n",
       "4           alpha     2     5  None     5     5     2     2     2     5  ...   \n",
       "...           ...   ...   ...   ...   ...   ...   ...   ...   ...   ...  ...   \n",
       "44995       omega  None  None  None  None  None  None  None  None  None  ...   \n",
       "44996       omega     1     3  None  None  None  None  None  None  None  ...   \n",
       "44997       omega  None  None  None  None  None  None  None  None  None  ...   \n",
       "44998       omega  None     1  None  None  None  None  None  None  None  ...   \n",
       "44999       omega  None  None  None  None  None     1  None  None  None  ...   \n",
       "\n",
       "      eco_860 eco_861 eco_862 eco_863 eco_864 eco_865 eco_866 eco_867 eco_868  \\\n",
       "0        None    None    None    None    None    None    None    None    None   \n",
       "1        None    None    None    None    None    None    None    None    None   \n",
       "2        None    None    None    None    None    None    None    None    None   \n",
       "3        None    None    None    None    None    None    None    None    None   \n",
       "4        None    None    None    None    None    None    None    None    None   \n",
       "...       ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "44995    None    None    None    None    None    None    None    None    None   \n",
       "44996    None    None    None    None    None       4    None    None    None   \n",
       "44997    None    None    None    None    None    None    None    None    None   \n",
       "44998       3    None    None       4    None    None    None    None    None   \n",
       "44999    None    None    None    None    None    None    None    None    None   \n",
       "\n",
       "      eco_869  \n",
       "0        None  \n",
       "1        None  \n",
       "2        None  \n",
       "3        None  \n",
       "4        None  \n",
       "...       ...  \n",
       "44995    None  \n",
       "44996    None  \n",
       "44997    None  \n",
       "44998    None  \n",
       "44999    None  \n",
       "\n",
       "[45000 rows x 870 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hacemos una copia del dataframe de clientes_lectura\n",
    "clientes_rank = copy.deepcopy(clientes_lectura)\n",
    "\n",
    "# Extraemos el tipo de suscripción de cada cliente\n",
    "tipo_susc = clientes_rank.pop('suscripcion')\n",
    "\n",
    "# Máscara que filtra cuáles sí han sido leídas\n",
    "filtro_lectura = (clientes_rank == 1)\n",
    "\n",
    "# A las noticias que NO han sido leídas le asignamos una puntuación de None (nulo)\n",
    "clientes_rank[~filtro_lectura] = None\n",
    "\n",
    "# Probabilidad de que un cliente puntúe una noticia tras haberla leído\n",
    "p = 0.8\n",
    "\n",
    "# Segundo filtro; asignamos valor nulo a aquellas noticias que sí han sido leídas pero no puntuadas\n",
    "clientes_rank[filtro_lectura] = np.where(np.random.random(clientes_rank[filtro_lectura].shape) < (1-p), None, clientes_rank[filtro_lectura])\n",
    "\n",
    "# Unificamos la nomenclatura de nulos; hasta aquí tenemos NaN y None. Pasamos todos los nulos a None\n",
    "clientes_rank = clientes_rank.where(pd.notnull(clientes_rank), None)\n",
    "\n",
    "# Tercer filtro: noticias que sí han sido puntuadas, tras haber filtrado las que han sido leídas pero no puntuadas\n",
    "filtro_lectura2 = (clientes_rank == 1)\n",
    "\n",
    "# De esas que han sido puntuadas, le asignamos una puntuación de 1 a 5 (como si fuesen estrellas? puntuación 1<2<3<4<5)\n",
    "clientes_rank[filtro_lectura2] = np.random.randint(1,6, size=(45000, len(columnas)))\n",
    "\n",
    "# Dataframe final con las puntuaciones\n",
    "clientes_rank.insert(0, 'suscripcion', tipo_susc)\n",
    "clientes_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        306\n",
       "1        321\n",
       "2        324\n",
       "3        336\n",
       "4        312\n",
       "        ... \n",
       "44995     73\n",
       "44996     58\n",
       "44997     66\n",
       "44998     68\n",
       "44999     69\n",
       "Length: 45000, dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cuántas noticias ha puntuado cada uno (se observa que es < al número de noticias leídas)\n",
    "clientes_rank.count(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exportar a csv\n",
    "nombre = 'clientes_susc_rank_' + datetime.today().strftime('%d%m%Y') + '.csv'\n",
    "clientes_rank.to_csv(nombre, encoding='utf-8-sig', header = True, sep = ';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por tanto, ya tendríamos dos dataframes de clientes:  \n",
    "  \n",
    "**clientes_lectura**: solo con valores 1 (si ha leído la noticia) y 0 (si no la ha leído).  \n",
    "**clientes_rank**: con las puntuaciones que han dado a las noticias."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
